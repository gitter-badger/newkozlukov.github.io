<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://newkozlukov.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://newkozlukov.github.io/" rel="alternate" type="text/html" /><updated>2018-01-28T06:59:34+03:00</updated><id>https://newkozlukov.github.io/</id><title type="html">NEWKozlukov</title><subtitle>Noch ein weiterer [Serge] Kozlukov.&lt;br/&gt;
A place for notes on learning math,
naive opinions and inappropriate proclamations.
</subtitle><entry><title type="html">The Right to Offend</title><link href="https://newkozlukov.github.io/social/2018/01/25/the-right-to-offend/" rel="alternate" type="text/html" title="The Right to Offend" /><published>2018-01-25T00:00:00+03:00</published><updated>2018-01-25T00:00:00+03:00</updated><id>https://newkozlukov.github.io/social/2018/01/25/the-right-to-offend</id><content type="html" xml:base="https://newkozlukov.github.io/social/2018/01/25/the-right-to-offend/">&lt;p&gt;It’s January 25, 2018 as I’m writing this little essay.
A few days ago a terrible news struck the Russian students.
Some mental kid (Artyom, BMSTU) had in cold blood strangled to death, stabbed
and raped his girl roommate (Tanya, HSE). He had moreover written a detailed
description of the process and &lt;a href=&quot;https://web.archive.org/web/20180123204415/http://telegra.ph/Istoriya-Artyoma-18-01-23&quot;&gt;published it
online&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is a nasty case and the only thing nastier is that these things feel normal
in this country. But what is more interesting is the response of the public.
Just as one would expect there happend to be quite a lot of people who having
discovered that the slain girl wasn’t an innocent angel have put the blame on
her:
she used to drink,
she posted erotic pics online,
she’d been demonstratively ignoring Artyom who loved her,
she’d repeatedly provoked him,
and so on.
This opinion had in many ways been expressed on the web
and was later picked up by the toxic jerks of Russian TV.
I hope I don’t need to say that it’s a bullshit.&lt;/p&gt;

&lt;h2 id=&quot;the-principle&quot;&gt;The principle&lt;/h2&gt;

&lt;p&gt;Now back to the point. This happening reminded me of some idea.
See our two poor heros were having an affair.
They’d been together for some time, they’d split up, and yet they’d lived
together after that. And all this time they’d been in a constant fight,
expressing disdain of, pinning, and punching one another.
And you know what? It sounds nice and sweet if you remove the finale (although
some might appreciate the whole play). It sounds lively.&lt;/p&gt;

&lt;p&gt;Think of it. Had we prohibited all offence what a world we’d live in?
Basically our every action can be considered by someone as offensive.
For instance when you hook up with a new person — you’re breaking into their
live. Just consider this situation:
you’re lost in an unfamiliar country, you’ve got no home, you’ve got no money,
you don’t speak the local language, and there isn’t a soul out there to help
you. Suddenly you encounter a man speaking english on the phone. The man’s
evidently in a great hurry (for some life-or-death meeting in a different town;
he’s been heading towards the train station actually; you aren’t aware of any of
that though).
You approach the man and ask him for help (for money, directions, oversleep).
But this way you impose a great amount of new responsibilities on him which he
never wanted. He doesn’t have to help you but if he denies you that’d make him
ill-feeling for now he knows about your situation. It could also have turned out
that the man’s willing to help. But he’s late and he won’t be able to come back
and help you in time. He has to choose now between his trouble and yours.
And here are a few questions in relation to this thought situation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Did you have the right to approach the man?&lt;/li&gt;
  &lt;li&gt;Does the man have the right to reject?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we are to prohibit an offence, we’re to prohibit any kind of interaction
between people. It’s a dead end just like absolute metaphysical skepticism.
As an answer to the two questions above I’d propose the following thesis:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is an essential human’s right to offend and be offended.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As for defining an appropriate behaviour I’d seek for some variational principle
rather than for strict’n’rigid boundary conditions.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Had Tanya been “provoking”? Her Instagramm says so. And hell she had right to!
Did Artyom have right to feel offended? Why, yes! And it’s perfectly fine! The
only problem here is that unfortunately Artyom broke. I can totally see why he
did and I’m sorry for both of them. Now what could’ve prevented him breaking? I
say a healthy society! But in the post-soviet Russia you’ve only got a toxic
hypocrite filth. I stop now and say no more.&lt;/p&gt;</content><author><name></name></author><category term="social" /><summary type="html">It’s January 25, 2018 as I’m writing this little essay. A few days ago a terrible news struck the Russian students. Some mental kid (Artyom, BMSTU) had in cold blood strangled to death, stabbed and raped his girl roommate (Tanya, HSE). He had moreover written a detailed description of the process and published it online.</summary></entry><entry><title type="html">Variational calculus, criteria (INPROGRESS)</title><link href="https://newkozlukov.github.io/cheat/2018/01/19/var-calc-criteria/" rel="alternate" type="text/html" title="Variational calculus, criteria (INPROGRESS)" /><published>2018-01-19T00:00:00+03:00</published><updated>2018-01-19T00:00:00+03:00</updated><id>https://newkozlukov.github.io/cheat/2018/01/19/var-calc-criteria</id><content type="html" xml:base="https://newkozlukov.github.io/cheat/2018/01/19/var-calc-criteria/">Consider a general problem
\[ J(x) = \int_{t_0}^{t_1} f_0(t, x(t), \dot x(t) \mathrm{d}t +
\Phi_0(t_0, t_1, x(t_0), x(t_1)) \longrightarrow \inf, \]
subject to
\[ \Phi(t_0, t_1, x(t_0), x(t_1)) = 0, \]
\[ x\in \mathrm{PC}^{(1)}(\mathbb{R}, \mathbb{R}^n). \]

Automagically reduces to optimal control problem.

&lt;h2&gt;Euler's equation&lt;/h2&gt;

&lt;h2&gt;Legendre's criterion&lt;/h2&gt;
\[ \frac{\partial^2}{\partial u^2} H(t, \cdots)\left.\right\vert_{x_\star, u_\star} \geq 0 \]
or equivalently:
\[ \lambda_0 \frac{\partial^2}{\partial {\dot x}^2} f_0(t, \cdots) \geq 0\ \text{at}
\ x_\star, \dot x_\star. \]

&lt;h2&gt;Weierstrass's criterion&lt;/h2&gt;
\[ \mathcal{I}(u) \geq \mathcal{I}(v) + \langle \mathcal{I}^\prime(v), u \rangle \]

&lt;h2&gt;Jacobi's criterion&lt;/h2&gt;</content><author><name></name></author><category term="cheat" /><summary type="html">Consider a general problem \[ J(x) = \int_{t_0}^{t_1} f_0(t, x(t), \dot x(t) \mathrm{d}t + \Phi_0(t_0, t_1, x(t_0), x(t_1)) \longrightarrow \inf, \] subject to \[ \Phi(t_0, t_1, x(t_0), x(t_1)) = 0, \] \[ x\in \mathrm{PC}^{(1)}(\mathbb{R}, \mathbb{R}^n). \]</summary></entry><entry><title type="html">Optimal control, criteria (INPROGRESS)</title><link href="https://newkozlukov.github.io/cheat/2018/01/19/opt-criterions/" rel="alternate" type="text/html" title="Optimal control, criteria (INPROGRESS)" /><published>2018-01-19T00:00:00+03:00</published><updated>2018-01-19T00:00:00+03:00</updated><id>https://newkozlukov.github.io/cheat/2018/01/19/opt-criterions</id><content type="html" xml:base="https://newkozlukov.github.io/cheat/2018/01/19/opt-criterions/">&lt;p&gt;
    Cheatsheet for Zadorozhnii' optimal control course.
    So the credit for whatever's correct goes to Zadorozhnii
    and all the errors are to be considered mine.
    Some refernces (specific to the course mentioned):
    &lt;ol&gt;
        &lt;li&gt;J. Warga: https://libgen.pw/item/detail/id/1199754?id=1199754&lt;/li&gt;
    &lt;/ol&gt;
&lt;/p&gt;

&lt;p&gt;
    Consider a general problem
    \[ J(x, u, t_0, t_1) = \int_{t_0}^{t_1} f_0(t, x(t), u(t) \mathrm{d}t +
    \Phi_0(t_0, t_1, x(t_0), x(t_1)) \longrightarrow \inf, \]
    subject to
    \[ \dot x(t) = f(t, x(t), u(t)), \]
    \[ \Phi(t_0, t_1, x(t_0), x(t_1)) = 0. \]
    \[ u\in\mathrm{PC}(\mathbb{R}, \mathbb{R}^n). \]
&lt;/p&gt;

&lt;h2&gt;Lagrangian&lt;/h2&gt;
&lt;p&gt;
    As usual we use the Lagrange's multiplier method
    in order to reduce the bounded problem to an unbounded one:
    \[ L(u, \lambda_0, \lambda, \mu)
    = \lambda_0 J(x, u)
    + \int_{t_0}^{t_1} \langle \lambda(t),
    \left[ f(t, x(t), u(t)) - \dot x(t)\right]
    \rangle \mathrm{d}t
    + \langle \mu, \Phi(t_0, t_1, x(t_0), x(t_1)).\]

&lt;/p&gt;

&lt;h2&gt;Hamiltonian&lt;/h2&gt;
&lt;p&gt;
    Let us introduce the helper functions with the formulas
    \[ H(t, x(t), u(t), \lambda_0, \lambda(t), \mu)
    = \lambda_0 f_0(t, x(t), u(t))
    + \langle \lambda(t), f(t) \rangle, \]
    \[ l(t_0, t_1, x(t_0), x(t_1), \mu)
    = \langle \mu, \Phi(t_0, t_1, x(t_0), x(t_1)) \rangle. \]
    Now we can rewrite the Lagrangian:
    \[ L(\cdots)
    = \int_{t_0}^{t_1} \left[
    H(\cdots)
    + \langle \dot \lambda(t), x(t) \rangle
    \right] \mathrm{d}t
    + l(\cdots)
    + \langle \lambda(t_0), x(t_0) \rangle
    - \langle \lambda(t_1), x(t_1) \rangle. \]
    Here
    \[
    \langle \lambda(t_0), x(t_0) \rangle
    - \langle \lambda(t_1), x(t_1) \rangle =
    \int_{t_0}^{t_1} \langle \lambda(t), -\dot x(t)\rangle \mathrm{d}t. \]
&lt;/p&gt;</content><author><name></name></author><category term="cheat" /><summary type="html">Cheatsheet for Zadorozhnii' optimal control course. So the credit for whatever's correct goes to Zadorozhnii and all the errors are to be considered mine. Some refernces (specific to the course mentioned): J. Warga: https://libgen.pw/item/detail/id/1199754?id=1199754</summary></entry><entry><title type="html">Langauge-related things I want to learn some time and reasons to</title><link href="https://newkozlukov.github.io/lang,/drafts/2017/12/19/lang-todo/" rel="alternate" type="text/html" title="Langauge-related things I want to learn some time and reasons to" /><published>2017-12-19T00:00:00+03:00</published><updated>2017-12-19T00:00:00+03:00</updated><id>https://newkozlukov.github.io/lang,/drafts/2017/12/19/lang-todo</id><content type="html" xml:base="https://newkozlukov.github.io/lang,/drafts/2017/12/19/lang-todo/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Overview of graph spectra stuff</title><link href="https://newkozlukov.github.io/math/2017/12/19/graphs-spectra/" rel="alternate" type="text/html" title="Overview of graph spectra stuff" /><published>2017-12-19T00:00:00+03:00</published><updated>2017-12-19T00:00:00+03:00</updated><id>https://newkozlukov.github.io/math/2017/12/19/graphs-spectra</id><content type="html" xml:base="https://newkozlukov.github.io/math/2017/12/19/graphs-spectra/">&lt;section&gt;
    &lt;h3&gt;Structure&lt;/h3&gt;

    &lt;ul&gt;
        &lt;li&gt;Which graphs exactly?&lt;/li&gt;
        &lt;li&gt;Which spectra?&lt;/li&gt;
        &lt;li&gt;Why?&lt;/li&gt;
    &lt;/ul&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Which graphs?&lt;/h3&gt;
    &lt;p&gt;
    A &quot;graph&quot; hereafter is a simple directed graph
    with the finite set of vertices $V=\left\{1,\ldots,N\right\}$
    and the set of directed edges $E \subset V^2$.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Adjacency matrices&lt;/h3&gt;
    &lt;p&gt;
    Such a graph is uniquely defined by its &lt;i&gt;adjacency matrix&lt;/i&gt;
    $$A = (a_{ij}),\ i,j{=}\overline{1,N},$$
    where $a_{ij}$
    is the number of directed edges
    from vertex $j$ to vertex $i$.&lt;br/&gt;
    Note the inversion of indices.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Simple graphs&lt;/h3&gt;
    &lt;p&gt;
    Sometimes we'll restrict ourselves to &lt;i&gt;simple graphs&lt;/i&gt;:
    the ones without loops and multiple edges.
    In terms of adjacency matrix:
    $$a_{ij}\in \{0,1\},\ a_{ii} = 0,$$
    for $i,j = \overline{1,N}.$
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Which spectra?&lt;/h3&gt;

    &lt;p&gt;
    We are interested in
    &lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;the spectrum of the adjacency matrix $A$&lt;/li&gt;
        &lt;li&gt;and the spectra of transition matrices of some random processes.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The (Bonacich) eigenvector centrality&lt;/h3&gt;

    &lt;p&gt;Consider a problem of ranking
    scientific papers or web-pages
    that reference each other.&lt;/p&gt;
    &lt;ol&gt;
        &lt;li&gt;The more links to a page the greater importance.&lt;/li&gt;
        &lt;li class=&quot;fragment&quot;&gt;The more important referrers the greater importance.&lt;/li&gt;
        &lt;li class=&quot;fragment&quot;&gt;Iterate.&lt;/li&gt;
        &lt;li class=&quot;fragment&quot;&gt;$$x_j = c \sum_i a_{ik} x_k.$$&lt;/li&gt;
    &lt;/ol&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;p&gt;In more convenient form:
    $$A x = \lambda x.$$&lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    For the importances to be non-negative
    a principal eigenvector is chosen.&lt;br/&gt;
    That is one corresponding to the largest (the Perron-Frobenius) eigenvalue.&lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;For a strongly-connected simple graph
    the adjacency matrix is always irreducible and Perron-Frobenius applies.&lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Google's PageRank: random surfer interpretation&lt;/h3&gt;

    &lt;p&gt;An alternative approach to define eigenvector centrality.
    Consider an agent visiting a web page
    and then randomly following one of the links.&lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;It's a Markovian random walk with the probability
    of transition from state (web-page) $j$
    to state $i$ equal to $$p_{ij} = \frac{a_{ij}}{D_j}.$$
    Here $D_j = \sum_k a_{kj}$ is the out-degree of the node $j$.&lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Random surfer interpretation&lt;/h3&gt;

    &lt;p&gt;
    Let $D=\mathrm{diag}(d_1,\ldots d_N)$.&lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    Then $P=AD^{-1}$ is the (column-stochastic)
    transition matrix of this process.&lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    Its Perron-Frobenius eigenvalue is $1$.
    It has a non-negative eigenvector
    that defines a stationary distribution:
    $$x = P x,$$
    $x\geq 0,\ \sum_j x_j = 1$.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    Interpretation:
    the fraction of time a random surfer would spend in each node&lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The second eigenvalue of $P$&lt;/h3&gt;

    &lt;p&gt;Customarily the eigenvector centrality is computed
    using the power iteration method.
    For a &lt;b&gt;positive&lt;/b&gt; transition matrix $P$ (Perron-Frobenius):
    &lt;ol&gt;
        &lt;li&gt;Start at arbitrary $x^{(0)}\neq 0$.&lt;/li&gt;
        &lt;li&gt;Set
            $x^{(n)} = \frac{1}{\|x^{(n-1)}\|} P x^{(n-1)},\ n\in\mathbb{N}.$
        &lt;/li&gt;
    &lt;/ol&gt;
    &lt;/p&gt;
    &lt;p&gt;
    The process converges to a strictly positive eigenvector $x$
    which corresponds to the eigenvalue $1$.
    This eigenvalue is simple and equals the spectral radius of $P$.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The second eigenvalue of $P$&lt;/h3&gt;

    &lt;p&gt;
    First, it is obvious that for positive $P$:
    $$\|x - x^{(n)}\| = O(\lvert\frac{\lambda_2}{\lambda_1}\rvert^n) = O(\lvert\lambda_2\rvert^n),$$
    where $\lambda_2$ is the second-largest eigenvalue of $P$.&lt;br/&gt;
    In other words the speed of convergence is determined by $\lambda_2$.&lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    Meyer (1994) also shows that (for non-negative $P$)
    the sensitivity of stationary distributions
    to perturbations in P
    is determined by the gap
    between the two largest eigenvalues.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Convex combinations&lt;/h3&gt;

    &lt;p&gt;The Page's paper also deals with the disconnectedness of the graph of web.
    It is done by allowing an agent (a random surfer)
    to reset and jump to an arbitrary page
    with some probability $\alpha$.
    &lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    The transition matrix for this new process is:
    $$\mathcal{H} = (1-\alpha)P + \alpha J,$$
    where $J$ is the all-ones matrix.
    &lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    This matrix is positive and thus the Perron-Frobenius theorem
    and the power method are applicable!
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Viral spread.&lt;/h3&gt;

    &lt;p&gt;
    The Susceptible-Infective-Susceptible model
    describes viral spread in a network on $N$ nodes
    as a Markov process with $2^N$ states.
    Specifically, state is a vector of nodes' statuses
    (each node is either infected or susceptible to infection).&lt;br/&gt;
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Viral spread.&lt;/h3&gt;

    &lt;p&gt;
    Wang et al. investigated ergodicity of such a system
    in the following case:
    &lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;An infected node gets cured with the probability $\delta$.&lt;/li&gt;
        &lt;li&gt;A healthy neighbour of infective node gets infected w.p. $\beta$.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Viral spread.&lt;/h3&gt;

    &lt;p&gt;
    Turns out (Wang et. al):
    $$\mathrm{Pr}\left\{\text{node}\ i\ \text{is infected at the step}\ n\right\} \to 0,\ \text{as}\ n\to\infty,$$
    for all $i$
    if (and only if) $$\frac{\beta}{\delta} &lt; \frac{1}{\mathrm{spr}A}.$$
    Moreover this probability decays in exponential manner.
    &lt;/p&gt;
    &lt;p&gt;
    Otherwise infection lingers for ever.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Viral spread.&lt;/h3&gt;
    &lt;p&gt;
    TLDR: the spectral radius of the adjacency matrix
    represents the level of inter-connectedness of a network.
    &lt;/p&gt;
&lt;/section&gt;
&lt;!-- TODO
    -   Spectral clustering, NNMF
    -   Almost-invariant sets
--&gt;
&lt;section&gt;
        &lt;h3&gt;Kronecker products&lt;/h3&gt;

        &lt;p&gt;
        One of the most general operations on graphs
        &lt;i&gt;is the non-complete extended p-sum&lt;/i&gt; (NEPS) is defined as follows (see Cvetkovic):
        &lt;/p&gt;
        &lt;p class=&quot;fragment&quot;&gt;
        The NEPS of graphs $G_1,\ldots, G_n$
        with basis $\mathcal{B}\subset \{0,1\}^n\setminus 0$
        is the graph with vertex set $V(G_1)\times\cdots\times V(G_n)$,
        in which a vertex $u=(u_1,\ldots,u_n)$ is linked to a vertex $v=(v_1,\ldots,v_n)$
        IFF there is $\beta=(\beta_1,\ldots,\beta_n)\in\mathcal{B}$
        such that
        $u_i=v_i$ whenever $\beta_i=0$
        and $u_i$ is linked to $v_i$ in $G_i$ whenever $\beta_i=1$.
        &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;
    
    &lt;p&gt;
    NEPS can serve a basis for some other important operations.
    &lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;The cartesian product of two graphs: $\mathcal{B}=\{(0,1),\ (1,0)\}$.&lt;/li&gt;
        &lt;li&gt;The strong product: $\mathcal{B}=\{ (0,1),\ (1, 0),\ (1,1) \}$.&lt;/li&gt;
        &lt;li&gt;etc.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    The adjacency matrix of the NEPS of $G_1,\ldots,G_n$
    with basis $\mathcal{B}$ is
    $$A = \sum_{\beta\in\mathcal{B}} A_1^{\beta_1} \otimes\cdots\otimes A_n^{\beta_n}$$
    where $A_1,\ldots,A_n$ are adjacency matrices of $G_1,\ldots, G_n$ resp.
    and $\otimes$ denotes the Kronecker product (see below).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    The Kronecker product of two matrices $A=(a_{ij})\in\mathbb{C}^{m{\times}n}$
    and $B=(b_{ij})\in\mathbb{C}^{p{\times}q}$ is the matrix of the size $mn{\times}pq$
    defined as:
    $$A{\otimes}B =
    \begin{pmatrix}
    a_{11} B &amp; \cdots &amp; a_{1n} B \\
    \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} B &amp; \cdots &amp; a_{mn} B \\
    \end{pmatrix}.$$
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;Kronecker product has some convenient properties:&lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt; It is associative:
            $A\otimes (B\otimes C) = (A\otimes B)\otimes C.$&lt;/li&gt;
        &lt;li class=&quot;fragment&quot;&gt; It is distributive w.r.t. addition:
            $(A+B)\otimes(C+D) = A\otimes C + A\otimes D +$&lt;/br&gt;$+ B\otimes C + B\otimes D.$&lt;/li&gt;
        &lt;li class=&quot;fragment&quot;&gt;$(AB)\otimes(CD) = (A\otimes C)(B\otimes D)$
            whenever products \( AB \) and \( CD \) make sense.
            (I.e. it corresponds to tensor products of operators)
        &lt;/li&quot;&gt;
        &lt;li class=&quot;fragment&quot;&gt;
            $\operatorname{tr}(A\otimes B) = \operatorname{tr}A\operatorname{tr}B$.&lt;/li&gt;
        &lt;li class=&quot;fragment&quot;&gt; $A$ and $B$ are symmetric $\implies$
            $A\otimes B$ is symmetric.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/section&gt;</content><author><name></name></author><summary type="html">A part of my talk on current-problems-2017 dedicated to the graph spectra applications</summary></entry><entry><title type="html">CONF2017: The method of similar operators in the study of spectra of the adjacency matrices of graphs</title><link href="https://newkozlukov.github.io/math/2017/12/19/conf-2017/" rel="alternate" type="text/html" title="CONF2017: The method of similar operators in the study of spectra of the adjacency matrices of graphs" /><published>2017-12-19T00:00:00+03:00</published><updated>2017-12-19T00:00:00+03:00</updated><id>https://newkozlukov.github.io/math/2017/12/19/conf-2017</id><content type="html" xml:base="https://newkozlukov.github.io/math/2017/12/19/conf-2017/">&lt;section&gt;
    &lt;h2&gt;The method of similar operators in the study of spectra of the adjacency matrices of graphs&lt;/h2&gt;

        &lt;p&gt;
        Serge Kozlukov
    &lt;/p&gt;
    &lt;a href=&quot;mailto:newkozlukov@gmail.com&quot;&gt;newkozlukov@gmail.com&lt;/a&gt;
    &lt;p&gt;Voronezh SU, Department of nonlinear oscillations&lt;/p&gt;
    &lt;!--&lt;p&gt;GPG:  0E836B03166B876AD13FFC42&lt;b&gt;FCE234D33DC319EA&lt;/b&gt;&lt;/p&gt;--&gt;

&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;REMOVED: a whole lot of irrelevant examples&lt;/h3&gt;

    &lt;p&gt;
    The actual talk was a damn failure.
    I'll leave the relevant part of slides here
    and perhaps write an overview of graphs spectra
    (in the context of networks) for a blog&lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;The method of similar operators
    is a means to track eigenvalues
    and spectral projections
    of small-norm perturbations of some &quot;ideal&quot; operator.
    &lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    The method originates from Friedrichs (1965)
    and was later developed in abstract setting
    by Anatoly Baskakov (1983, 2013, 2017).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    Unlike usual perturbation theory methods
    which rely on series expansions,
    the method of similar operators
    makes use of fixed-point theorem
    in appropriate Banach space.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    First we introduce some notation.
    While the abstract method of similar operators
    was developed for Banach spaces,
    we will for the purposes of this talk,
    account for finite-dimensionality and
    put everything into matrix form.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    Two matrices \( A, B \)
    are called &lt;i&gt;similar&lt;/i&gt;
    if there is an invertible matrix \( U \)
    (the similarity matrix)
    such that \( A U = U B \).

    Similar matrices have identical spectrum
    and $U$ transforms eigenvectors of one to anothers':
    $Bx = \lambda x \implies A Ux = \lambda Ux$.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    We'll make use of the following spaces:
    &lt;/p&gt;
    &lt;ul&gt;
    &lt;li&gt;The Euclid space $\mathbb{C}^{n}$ with the $L_2$-norm.&lt;/li&gt;
    &lt;li&gt;The Banach algebra $L(V)$ of (bounded) linear operators $V\to V$
        defined on a Banach space $V$
        with the operator norm:
    \(
        \|A\|_{\mathrm{op}} =
        \sup_{
            \substack{\|x\|=1,\\ x\in V}
        } \|A x\|,\ A\in L(V).
        \)
    &lt;/ul&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;The space of matrices $\mathbb{C}^{n{\times}n}$
        of the size $n{\times}n$ with complex entries
        and one of the following norms:
        $\|A\|_{\mathrm{F}} = \sqrt{\sum_{ij} |a_{ij}|^2},$
        $\|A\|_{\mathrm{op}} =
        \sup_{
        \substack{\|x\|_2=1,\\ x\in \mathbb{C}^n}
            } \|A x\|_2,\ A\in \mathbb{C}^{m{\times}n}.$
    &lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
    NB: $\mathbb{C}^{n{\times}n}$ supplied
    with the operator norm $\|\cdot\|_{\mathrm{op}}$
    is isomorphic to the space $L(\mathbb{C}^n)$
    of linear operators $\mathbb{C}^{n}\to\mathbb{C}^{n}$.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    We will also consider the space
    $L(L(V))$
    of linear transformations of operators
    which we will refer to as &quot;transformers&quot; (following MG Krein).&lt;br/&gt;
    We will supply this space with operator norm
    corresponding to the norm in $L(V)$.&lt;br/&gt;
    NB: $L(L(\mathbb{C}^n))$ is isomorphic to $L(\mathbb{C}^{n{\times}n})$.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;The formulation of the main theorem of the method of similar operators
    is built around the notion of an &lt;i&gt;admissible triple&lt;/i&gt;
    (which effectively becomes a pair in finite-dimensional spaces).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    Consider a matrix $A\in \mathbb{C}^n$.
    A tuple \( (\mathbb{C}^{n{\times}n}, J, \Gamma) \)
    is said form an &lt;i&gt;admissible triple&lt;/i&gt;
    for the matrix $A$ (Baskakov) if
    &lt;ol&gt;
        &lt;li&gt;\( J, \Gamma \in L(\mathbb{C}^{n{\times}n}) \)
        are transformers.&lt;/li&gt;
        &lt;li&gt;\( J \) is a projection (\( J^2 = J \)).&lt;/li&gt;
        &lt;li&gt;
            $\Gamma$ satisfies&lt;br/&gt;
        \(
            A \Gamma X - (\Gamma X) A = X - JX,
        \)&lt;br/&gt;
        \(
            J\Gamma X = 0,
        \)
        for all $X\in\mathbb{C}^{n{\times}n}$.
        &lt;/li&gt;
    &lt;/ol&gt;
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    Consider a matrix \( A - B \),
        with \( A, B \in \mathbb{C}^{n{\times}n} \).
    Suppose \( (\mathbb{C}^{n{\times}n}, J, \Gamma) \)
        is admissible for \( A \)
        and suppose
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    Then there exists such a \( X^o\in\mathbb{C}^{n{\times}n} \)
        that \( A - B \) is similar to \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|,
        \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The method of similar operators&lt;/h3&gt;

    &lt;p&gt;
    \( X^o \) is the limit of a convergent sequence
        \( \left( \Phi^k(0);\ k\in\mathbb{N} \right) \)
        in a Banach algebra \( \mathbb{C}^{n{\times}n} \).
        Here \( \Phi \) is a non-linear contraction mapping
        defined in the ball \( \{X\in\mathbb{C}^{m{\times}n};\ \|X-B\|\leq 3\|B\| \} \)
        and given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B.
    \]
        Here \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes composition.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
    Consider a digraph defined by the following adjacency matrix:
    \[
        A = J_N - B =
        \begin{pmatrix}1 &amp; \cdots &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; \cdots &amp; 1\end{pmatrix} - B,
    \]
The unity on the intersection
of \( i \)'th row and \( j \)'th column of \( B \)
corresponds to an edge from \( j \) to \( i \)
being absent in the graph.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
The minimal annihilating polynomial of \( J_N \)
    is \( \lambda(\lambda - N) \).
This comes from the fact that \( J_N^2 = N J_N \).
Consequently the spectrum of \( J_N \) is
\[
    \sigma(J_N) = \left\{0, N\right\}.
\]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
The only non-zero eigenvalue \( N \) has the corresponding eigenvector
\[
    h_N = \frac{1}{\sqrt{N}} \left(1, \ldots, 1\right)\in\mathbb{R}^N.
\]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
The null-space of \( J_N \) is orthogonal to \( h_N \)
    and allows the orthonormal basis:
\[
    h_k = \frac{1}{\sqrt{k(k+1)}} \left(\underbrace{1, \ldots, 1}_{k\ \text{copies}}, -k, 0, \ldots, 0\right),
\]
for \( k=\overline{1, N-1} \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
We conclude that the adjacency matrix \( A = J_N - B \) of this graph
    is similar to \( \mathcal{A} - \mathcal{B} \)
    where
    \(
        \mathcal{A} = \left(\begin{array}{c|c}
        N &amp; \mathbf{0}_{1{\times}(N{-}1)} \\ \hline
            \mathbf{0}_{(N{-}1){\times}1} &amp; \mathbf{0}_{(N{-}1){\times}(N{-}1)}
        \end{array}\right) \in \mathbb{R}^{N{\times}N},
    \)
    and
    \(
        \mathcal{B} = U^{-1} B U \in \mathbb{R}^{N{\times}N}.
    \)
The similarity matrix \( U \) is given by stacking the eigenvectors in columns.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
Following the general method,
    we should first construct an admissible triple.
It is natural to set
    \[
        JX =
        \left(\begin{array}{c|c}
            x_{11} &amp; \mathbf{0} \\ \hline
            \mathbf{0} &amp; X_{22}
        \end{array}\right)
    \]
    &lt;/p&gt;
    &lt;p class=&quot;fragment&quot;&gt;
Then \( A - JX \) is block-diagonal for any \( X \)
    and its spectrum is the union
    of its diagonal blocks' spectra:
    \( \sigma(A - JX) = \{N, \sigma(-X_{22}) \} \).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
    Then (by definition of an admissible triple)
    \[
        \Gamma X = \frac{1}{N}
        \begin{pmatrix} 0 &amp; X_{12} \\ -X_{21} &amp; 0 \end{pmatrix},\ X\in\mathbb{C}^{N{\times}N}.
    \]

    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
    Then we estimate the norms:
    \( \|\Gamma\|_{\mathrm{op}} = \frac1N \).
    \( \|\mathcal{B}\|_{\mathrm{op}} =
    \|B\|_{\mathrm{op}} \leq
    \|B\|_{\mathrm{F}} =
    \sqrt{\sum_{ij} b_{ij}^2} = M \)&lt;br/&gt;
    Recall: $M$ is the number of absent edges.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
    Finally we have the result:
    &lt;/p&gt;
    &lt;p&gt;
    Suppose 
    \( M &lt; \frac{1}{16} N^2. \)
    Then
    \(
        \sigma(A) = \sigma_1 \cup \sigma_2,
    \)
    \( \sigma_1\cap \sigma_2 = \emptyset, \)
    where \( \sigma_1 = \{ N - x_{11}^o \} \)
    and
    \(
        \lvert x_{11}^o \rvert,
        \ \sup_{\lambda\in\sigma_2} \lvert\lambda\rvert \leq 4\sqrt{M}.
    \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;The results: almost-complete graph&lt;/h3&gt;

    &lt;p&gt;
    Moreover:
    \(
        \hat{h}_N = U(E+\Gamma X^o) e_1 = \)&lt;br/&gt;
    \( = h_N - \frac1N (X_{21,(1)}^o h_1 + \cdots + X_{21, (N{-}1)}^o h_{N{-}1}) \)
    is the principal eigenvector of $J_N - B$
    and
    \(
        \|\hat{h}_N - h_N\|_2 \leq 4\frac{\sqrt{M}}{N}.
    \)&lt;br/&gt;
    Here 
    \( X_{21,(i)}^o,\ i=\overline{1,N{-}1} \) are the coordinates
    of the vector \( X_{21}^o \).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    Now let \( A\in\mathbb{C}^{M{\times}M} \)
    and consider
    \(
        \mathbb{A} =
        \begin{pmatrix}
            A &amp; \cdots &amp; A \\
            \vdots &amp; \ddots &amp; \vdots \\
            A &amp; \cdots &amp; A
        \end{pmatrix}
        \in\mathbb{C}^{MN{\times}MN}
    \)
    and the perturbed matrix
    \(
        \mathbb{A} - \mathbb{B},\ \mathbb{B}\in\mathbb{C}^{MN{\times}MN}.
    \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    Suppose \( A \) is invertible self-adjoint matrix.
    Then it has \( M \) orthonormal eigenvectors \( h_1, \ldots, h_M \)
    (\(\left\|h_i\right\|_2 = 1,\ i{=}\overline{1,M}\))
    with the corresponding eigenvalues
    \( \lambda_1, \ldots, \lambda_M \neq 0\).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    The spectrum of \( \mathbb{A} \) is
    \(
        \sigma(\mathbb{A}) = \{0\}\cup N\sigma(A) = \{0\} \cup \{N\lambda;\ \lambda\in\sigma(A) \}.
    \)
    Non-zero eigenvalues of \( \mathbb{A} \)
        have corresponding block eigenvectors:
    \(
        f_j = \frac{1}{\sqrt{N}} (h_j, \ldots, h_j)\in \mathbb{C}^{MN},\ j=\overline{1,M}.
    \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    The null-space of \( \mathbb{A} \)
        has an orthonormal basis:
    \(
        f_{j,k} = \frac{1}{\sqrt{k(k+1)}}
        (
        \underbrace{e_j, \ldots, e_j}_{k\ \text{copies}},
        -ke_j,
        0, \ldots, 0
        ) \in\mathbb{C}^{MN{\times}M}
    \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    The matrix \( \mathbb{A} \) is similar to a block-diagonal matrix:
    \[
        \mathcal{A} =
        \left(\begin{array}{c|c}
            \operatorname{diag}(N\lambda_1,\ldots,N\lambda_M) &amp; \mathbf{0} \\ \hline
            \mathbf{0} &amp; \mathbf{0}
        \end{array}\right)\in\mathbb{C}^{MN{\times}MN};
    \]
    The similarity transform is orthogonal
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    We will consider block matrices
    of the size \( MN{\times}MN \)
    in the form
    \[
    X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &amp; \cdots &amp; x_{1M} \\
                \vdots &amp; \ddots &amp; \vdots \\
                x_{M1} &amp; \cdots &amp; x_{MM}
            \end{matrix} &amp;
            \begin{matrix}
                x_{1,M+1} \\
                \vdots \\
                x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                x_{M+1,1} &amp;
                \cdots &amp;
                x_{M+1,M}
            \end{matrix} &amp;
            X_{M+1,M+1}
        \end{array}\right),
    \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    As usual:
\(
        J X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &amp;  &amp; 0 \\
                 &amp; \ddots &amp;  \\
                0 &amp;  &amp; x_{MM}
            \end{matrix} &amp;
            \begin{matrix}
                0 \\
                \vdots \\
                0
            \end{matrix} \\ \hline
            \begin{matrix}
                0 &amp; \cdots &amp; 0
            \end{matrix} &amp;
            X_{M+1,M+1}
        \end{array}\right).
\)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    Suppose \( A \)
    is of simple spectrum (i.e. has distinct eigenvalues).
    Then
    \(\tiny
        \Gamma X = 
            \frac1n \left(\begin{array}{c|c}
            \begin{matrix}
                0               &amp; \gamma_{12}x_{12} &amp; \cdots &amp; \gamma_{1M}x_{1M} \\
                \gamma_{21}x_{21}  &amp; 0              &amp; \cdots &amp; \gamma_{2M}x_{2M} \\
                \vdots          &amp; \vdots         &amp; \ddots &amp; \vdots &amp; \ \\
                \gamma_{M1}x_{M1}  &amp; \gamma_{M2}x_{M2} &amp; \cdots &amp; 0
            \end{matrix} &amp;
            \begin{matrix}
                \gamma_{1,M+1}x_{1,M+1} \\
                \gamma_{2,M+1}x_{2,M+1} \\
                \vdots \\
                \gamma_{M,M+1}x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                \gamma_{M{+}1,1}x_{M{+}1,1} &amp;
                \gamma_{M{+}1,2}x_{M{+}1,2} &amp;
                \cdots &amp;
                \gamma_{M{+}1,M}x_{M{+}1,M}
            \end{matrix} &amp;
            \mathbf{0}
        \end{array}\right),
    \)
    \(\tiny
        \gamma_{ij} = \left\{
            \begin{aligned}
                &amp; \frac{1}{\lambda_i - \lambda_j},\ 1\leq i{\neq}j \leq M{+}1,\\
                &amp; 0,\ i=j
            \end{aligned}
            \right.
    \)&lt;br/&gt;
    with the convention
    \(\tiny
        \lambda_{M{+}1} = 0.
    \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    Then
    \(
        \|\Gamma\|_{\mathrm{op}} =
        \frac1N
        \frac{1}{\min\limits_{1\leq i{\neq}j \leq M{+}1}|\lambda_i - \lambda_j|} =
        \)&lt;br/&gt;
    \(
        = \frac1N
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|}},
         \frac{1}{
             \min\limits_{1\leq j \leq M}{|\lambda_j|}}
         \right\}.
    \)&lt;br/&gt;
    NB: the method of similar operators
    takes disjoint parts of spectrum
    and constructs a spectral projection for each.
    We've just relied on eigenvalues being distinct
    and our precision now depends
    on the smallest gap between two eigenvalues.
    Instead we could've taken the union
    of two eigenspaces.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
Suppose \( A \) has simple spectrum and the following inequality holds:
\(
    \left\| \mathbb{B} \right\|_{\mathrm{op}}
        \leq 
        \frac{N}{4}
         \min\left\{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|},
             \min\limits_{1\leq j \leq M}{|\lambda_j|}
         \right\}.
 \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
Then the spectrum of disturbed matrix \( \mathbb{A} - \mathbb{B} \) is
\[
    \sigma\left(\mathbb{A}\right) =
        \left\{
            N\lambda_1 - x_{11}^o, \ldots, N\lambda_M - x_{MM}^o
        \right\}
    \cup \sigma_{M{+}1},
\]
\[
    \lvert x_{jj}^o\rvert,
    \ \max_{\lambda\in\sigma_{M{+}1}} \lvert\lambda\rvert
    \leq 4\|B\|.
\]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;&quot;Tiled&quot; matrix example&lt;/h3&gt;

    &lt;p&gt;
    The eigenvectors
    \( \hat{f}_j,\ \hat{f}_{j,k},\ j{=}\overline{1,M},\ k{=}\overline{1,N{-1}} \)
    of the perturbed matrix satisfy
    \[
    \left\| \hat{f}_j - f_j \right\|_2,
    \ \left\| \hat{f}_{j,k} - f_{j,k}\right\|_2
    \leq
    \]
    \[ \leq
    \frac4N \|B\|
    \max\left\{
    \frac{1}{
    \min\limits_{1\leq l{\neq}p \leq M }{|\lambda_l - \lambda_p|}},
    \frac{1}{
    \min\limits_{1\leq l \leq M}{|\lambda_l|}}
    \right\}
    \]
    for all \( j{=}\overline{1,M}, k{=}\overline{1,N-1} \).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
Now we consider Kronecker product
\[
    A\otimes B =
    \begin{pmatrix}
        a_{11} B &amp; \cdots &amp; a_{1N} B \\
        \vdots   &amp; \ddots &amp; \vdots \\
        a_{N1} B &amp; \cdots &amp; a_{NN} B
    \end{pmatrix}
    \in \mathbb{C}^{MN{\times}MN}
\]
of squared matrices
\( A={(a_{ij})}\in\mathbb{C}^{N{\times}N},
 \ B={(b_{ij})}\in\mathbb{C}^{M{\times}M}. \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    We will analyze its spectral properties
    under small-norm perturbations:
    \begin{equation}\label{nkjpcs-kronperturb}
    A\otimes B - F.
    \end{equation}
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
Suppose \( A \) and \( B \) have simple structure,
    i.e. \( A \) has \( N \) eigenvectors
    \( f_1, \ldots, f_N \)
    whose corresponding eigenvalues are \( \mu_1, \ldots, \mu_N \)
    and \( B \) has eigenvectors \( h_1, \ldots, h_M \)
    with the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
Then \( A\otimes B \) also has simple structure;
    it has \( MN \) independent eigenvectors \( f_i\otimes h_j,\ i{=}\overline{1,N}, j{=}\overline{1,M} \)
    and the corresponding eigenvalues are \( \mu_i \lambda_j \).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
Now suppose that among these pairwise products \( \mu_i \lambda_j \)
    there are only \( s \) distinct values \( \nu_1, \ldots, \nu_s \).
To each eigenvalue \( \nu_k \) (\( k{=}\overline{1,s} \)) there corresponds
    an eigenspace \[ E_k = \operatorname{span}(f_i\otimes h_j;\ \mu_i\lambda_j = v_k,\ i{=}\overline{1,N},\ j{=}\overline{1,M}) \subset \mathbb{C}^{MN}. \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
These eigenspace form a direct-sum decomposition of \( \mathbb{C}^{MN} \):
    \[ \mathbb{C}^{MN} = E_1 \oplus \cdots \oplus E_s. \]
Any vector \( x\in\mathbb{C}^{MN} \) can be uniquely represented
    in the form
    \begin{equation}\label{nkjpcs-decomposition-x}
        x = x_1 + \cdots + x_s,\ x_k\in E_k,\ k=\overline{1,s}.
    \end{equation}
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
This direct-sum decomposition of \( \mathbb{C}^{MN} \)
    corresponds to a decomposition of the identity matrix \( E\in \mathbb{C}^{MN{\times}MN} \)
    (which defines an identity operator)
    into a sum of matrices of the spectral projections:
    \[
        E = P_1 + \cdots + P_s.
    \]
The spectral projection \( \mathcal{P}_k \) (\(k{=}\overline{1,s}\)) is given by the formula
    \[
        \mathcal{P}_k x = x_k \in E_k\subset \mathbb{C}^{MN}
    \]
    with respect to the decomposition~\eqref{nkjpcs-decomposition-x} of \( x \).
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
For any matrix \( X\in \mathbb{C}^{MN{\times}MN} \)
    the following trivial equality holds:
    \[
        X = \sum_{i,j=1}^s P_i X P_j.
    \]

The matrix \( A\otimes B \) can be decomposed into
    \[
        \mathcal{A} = \sum v_j P_j.
    \]

Now we should be able reproduce the same steps as before
    to retrieve the estimates.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
The natural way to define \( J \) is as follows:
    \[
        JX = \sum_{j=1}^s P_j X P_j.
    \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
The system of equations
    \[\left\{\begin{aligned}
        &amp; \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - JX, \\
        &amp; J\Gamma X = 0,\ X\in \mathbb{C}^{MN{\times}MN}
    \end{aligned}\right.\]
    has the unique solution:
    \[
        \Gamma X = \sum_{1\leq i{\neq}j \leq s} \frac{1}{\nu_i-\nu_j} P_i X P_j.
    \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    The norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} = \gamma = \frac{1}{\min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert}
    \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    Suppose
    \[
        \|F\| \leq \frac14 \gamma^{-1} = \frac14 \min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert.
    \]
    Then \( A{\otimes}B - F \) is similar to
    \[ \sum_{k=1}^s \nu_k P_k - JX^o = \sum_{k=1}^s (\nu_k P_k - P_k X^o P_k) \]
    for some \( X^o \in \mathbb{C}^{MN{\times}MN} \),
    \( \|X^o - F\|\leq 3\|F\| \).

    All the eigenvalues of \( A{\otimes}B - F \) are contained in the circles
    \[
        \Omega_k = \left\{
            \lambda\in\mathbb{C};
            \ \lvert\lambda - \nu_k\rvert \leq 4\|F\|
            \right\},
        \ k{=}\overline{1,s}.
    \]
    There is at least one eigenvalue in each of these circles.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    Suppose the eigenvalue \( \nu_k=\mu_{i_k}\lambda_{j_k} \) of \( A{\otimes}B \) has multiplicity \( 1 \),
        that is it has the only eigenvector \( v_k = f_{i_k}{\otimes}h_{j_k} \).
    It is equivalent to the statement that the eigenvalue \( \mu_{i_k} \)
        of \( A \) and the eigenvalue \( \lambda_{j_k} \) of \( B \)
        are both of multiplicity \( 1 \).
    Then \( A{\otimes}B - F \) has eigenvalue in the circle \( \Omega_k \)
        and the corresponding eigenvector \( \hat{v}_k \) is within bounds
    \[
        \|\hat{v}_k - v_k\| \leq 4\gamma \|F\|.
    \]
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
    If \( \nu_k \) is well separated from all the other eigenvalues of \( A{\otimes}B \):
    \[
        \min_{l\neq k}
        \lvert
        \nu_k - \nu_l
        \rvert
        \geq 4\|F\|,
    \]
    then \( \nu_k \) is the only eigenvalue of \( A{\otimes}B - F \)
    in that circle.
    &lt;/p&gt;
&lt;/section&gt;

&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
For example, in the case of the already-mentioned &quot;tiled&quot; matrix
\(
    J_N{\otimes}B =
    \begin{pmatrix}
        B &amp; \cdots &amp; B \\
        \vdots &amp; \ddots &amp; \vdots \\
        B &amp; \cdots &amp; B
    \end{pmatrix}
\)&lt;br/&gt;
    we would have
    \( \nu_1=N \),
    \( \nu_2=0 \).
Let \( \lambda_1,\ldots,\lambda_M \)
    be the eigenvalues of \( B \).
The spectrum of \( J_N{\otimes}B \) is
    \(
        \sigma(J_N{\otimes}B) = \left\{ \mu_i\lambda_j;\ i{=}\overline{1,2},\ j{=}\overline{1,M}\right\} = \{0\}\cup N\sigma(B). \)
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
All of these eigenvalues except for \( 0 \)
    are of multiplicity \( 1 \)
    and are well-separated for sufficiently large \( N \).
Then \( \gamma=\frac1N \).
This directly implies the theorem of the previous section.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;Kronecker products&lt;/h3&gt;

    &lt;p&gt;
These results might be refined
    with the use of the theorem on splitting an operator~\cite{baskakov1987theorem}
    which allows to consider each eigenvalue individually
    and obtain more precise estimates for the corresponding
    eigenvalue of the perturbed matrix.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h3&gt;References&lt;/h3&gt;

    &lt;p&gt;TODO&lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h2&gt;The method of similar operators in the study of spectra of the adjacency matrices of graphs&lt;/h2&gt;

        &lt;p&gt;
        Serge Kozlukov
    &lt;/p&gt;
    &lt;a href=&quot;mailto:newkozlukov@gmail.com&quot;&gt;newkozlukov@gmail.com&lt;/a&gt;
    &lt;p&gt;Voronezh SU, Department of nonlinear oscillations&lt;/p&gt;
    &lt;!--&lt;p&gt;GPG:  0E836B03166B876AD13FFC42&lt;b&gt;FCE234D33DC319EA&lt;/b&gt;&lt;/p&gt;--&gt;

&lt;/section&gt;</content><author><name></name></author><summary type="html">The math part of my current-problems-2017 talk</summary></entry><entry><title type="html">runlevel 6</title><link href="https://newkozlukov.github.io/2017/11/26/runlevel6/" rel="alternate" type="text/html" title="runlevel 6" /><published>2017-11-26T00:00:00+03:00</published><updated>2017-11-26T00:00:00+03:00</updated><id>https://newkozlukov.github.io/2017/11/26/runlevel6</id><content type="html" xml:base="https://newkozlukov.github.io/2017/11/26/runlevel6/">&lt;p&gt;I’m giving it another try
and rebooting the blog&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m giving it another try and rebooting the blog</summary></entry></feed>