---
title: "CONF2017: The method of similar operators in the study of spectra of the adjacency matrices of graphs"
categories: math
layout: slides
slides_theme: white
excerpt: |
  The math part of my current-problems-2017 talk
---

<section>
    <h2>The method of similar operators in the study of spectra of the adjacency matrices of graphs</h2>

    {% include my_contacts.html %}
</section>
<section>
    <h3>REMOVED: a whole lot of irrelevant examples</h3>

    <p>
    The actual talk was a damn failure.
    I'll leave the relevant part of slides here
    and perhaps write an overview of graphs spectra
    (in the context of networks) for a blog</p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>The method of similar operators
    is a means to track eigenvalues
    and spectral projections
    of small-norm perturbations of some "ideal" operator.
    </p>
    <p class="fragment">
    The method originates from Friedrichs (1965)
    and was later developed in abstract setting
    by Anatoly Baskakov (1983, 2013, 2017).
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Unlike usual perturbation theory methods
    which rely on series expansions,
    the method of similar operators
    makes use of fixed-point theorem
    in appropriate Banach space.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    First we introduce some notation.
    While the abstract method of similar operators
    was developed for Banach spaces,
    we will for the purposes of this talk,
    account for finite-dimensionality and
    put everything into matrix form.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Two matrices \( A, B \)
    are called <i>similar</i>
    if there is an invertible matrix \( U \)
    (the similarity matrix)
    such that \( A U = U B \).

    Similar matrices have identical spectrum
    and $U$ transforms eigenvectors of one to anothers':
    $Bx = \lambda x \implies A Ux = \lambda Ux$.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    We'll make use of the following spaces:
    </p>
    <ul>
    <li>The Euclid space $\mathbb{C}^{n}$ with the $L_2$-norm.</li>
    <li>The Banach algebra $L(V)$ of (bounded) linear operators $V\to V$
        defined on a Banach space $V$
        with the operator norm:
    \(
        \|A\|_{\mathrm{op}} =
        \sup_{
            \substack{\|x\|=1,\\ x\in V}
        } \|A x\|,\ A\in L(V).
        \)
    </ul>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>The space of matrices $\mathbb{C}^{n{\times}n}$
        of the size $n{\times}n$ with complex entries
        and one of the following norms:
        $\|A\|_{\mathrm{F}} = \sqrt{\sum_{ij} |a_{ij}|^2},$
        $\|A\|_{\mathrm{op}} =
        \sup_{
        \substack{\|x\|_2=1,\\ x\in \mathbb{C}^n}
            } \|A x\|_2,\ A\in \mathbb{C}^{m{\times}n}.$
    </p>
    <p class="fragment">
    NB: $\mathbb{C}^{n{\times}n}$ supplied
    with the operator norm $\|\cdot\|_{\mathrm{op}}$
    is isomorphic to the space $L(\mathbb{C}^n)$
    of linear operators $\mathbb{C}^{n}\to\mathbb{C}^{n}$.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    We will also consider the space
    $L(L(V))$
    of linear transformations of operators
    which we will refer to as "transformers" (following MG Krein).<br/>
    We will supply this space with operator norm
    corresponding to the norm in $L(V)$.<br/>
    NB: $L(L(\mathbb{C}^n))$ is isomorphic to $L(\mathbb{C}^{n{\times}n})$.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>The formulation of the main theorem of the method of similar operators
    is built around the notion of an <i>admissible triple</i>
    (which effectively becomes a pair in finite-dimensional spaces).
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Consider a matrix $A\in \mathbb{C}^n$.
    A tuple \( (\mathbb{C}^{n{\times}n}, J, \Gamma) \)
    is said form an <i>admissible triple</i>
    for the matrix $A$ (Baskakov) if
    <ol>
        <li>\( J, \Gamma \in L(\mathbb{C}^{n{\times}n}) \)
        are transformers.</li>
        <li>\( J \) is a projection (\( J^2 = J \)).</li>
        <li>
            $\Gamma$ satisfies<br/>
        \(
            A \Gamma X - (\Gamma X) A = X - JX,
        \)<br/>
        \(
            J\Gamma X = 0,
        \)
        for all $X\in\mathbb{C}^{n{\times}n}$.
        </li>
    </ol>
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Consider a matrix \( A - B \),
        with \( A, B \in \mathbb{C}^{n{\times}n} \).
    Suppose \( (\mathbb{C}^{n{\times}n}, J, \Gamma) \)
        is admissible for \( A \)
        and suppose
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Then there exists such a \( X^o\in\mathbb{C}^{n{\times}n} \)
        that \( A - B \) is similar to \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|,
        \]
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    \( X^o \) is the limit of a convergent sequence
        \( \left( \Phi^k(0);\ k\in\mathbb{N} \right) \)
        in a Banach algebra \( \mathbb{C}^{n{\times}n} \).
        Here \( \Phi \) is a non-linear contraction mapping
        defined in the ball \( \{X\in\mathbb{C}^{m{\times}n};\ \|X-B\|\leq 3\|B\| \} \)
        and given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B.
    \]
        Here \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes composition.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Consider a digraph defined by the following adjacency matrix:
    \[
        A = J_N - B =
        \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - B,
    \]
The unity on the intersection
of \( i \)'th row and \( j \)'th column of \( B \)
corresponds to an edge from \( j \) to \( i \)
being absent in the graph.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
The minimal annihilating polynomial of \( J_N \)
    is \( \lambda(\lambda - N) \).
This comes from the fact that \( J_N^2 = N J_N \).
Consequently the spectrum of \( J_N \) is
\[
    \sigma(J_N) = \left\{0, N\right\}.
\]
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
The only non-zero eigenvalue \( N \) has the corresponding eigenvector
\[
    h_N = \frac{1}{\sqrt{N}} \left(1, \ldots, 1\right)\in\mathbb{R}^N.
\]
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
The null-space of \( J_N \) is orthogonal to \( h_N \)
    and allows the orthonormal basis:
\[
    h_k = \frac{1}{\sqrt{k(k+1)}} \left(\underbrace{1, \ldots, 1}_{k\ \text{copies}}, -k, 0, \ldots, 0\right),
\]
for \( k=\overline{1, N-1} \)
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
We conclude that the adjacency matrix \( A = J_N - B \) of this graph
    is similar to \( \mathcal{A} - \mathcal{B} \)
    where
    \(
        \mathcal{A} = \left(\begin{array}{c|c}
        N & \mathbf{0}_{1{\times}(N{-}1)} \\ \hline
            \mathbf{0}_{(N{-}1){\times}1} & \mathbf{0}_{(N{-}1){\times}(N{-}1)}
        \end{array}\right) \in \mathbb{R}^{N{\times}N},
    \)
    and
    \(
        \mathcal{B} = U^{-1} B U \in \mathbb{R}^{N{\times}N}.
    \)
The similarity matrix \( U \) is given by stacking the eigenvectors in columns.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
Following the general method,
    we should first construct an admissible triple.
It is natural to set
    \[
        JX =
        \left(\begin{array}{c|c}
            x_{11} & \mathbf{0} \\ \hline
            \mathbf{0} & X_{22}
        \end{array}\right)
    \]
    </p>
    <p class="fragment">
Then \( A - JX \) is block-diagonal for any \( X \)
    and its spectrum is the union
    of its diagonal blocks' spectra:
    \( \sigma(A - JX) = \{N, \sigma(-X_{22}) \} \).
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Then (by definition of an admissible triple)
    \[
        \Gamma X = \frac{1}{N}
        \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix},\ X\in\mathbb{C}^{N{\times}N}.
    \]

    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Then we estimate the norms:
    \( \|\Gamma\|_{\mathrm{op}} = \frac1N \).
    \( \|\mathcal{B}\|_{\mathrm{op}} =
    \|B\|_{\mathrm{op}} \leq
    \|B\|_{\mathrm{F}} =
    \sqrt{\sum_{ij} b_{ij}^2} = M \)<br/>
    Recall: $M$ is the number of absent edges.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Finally we have the result:
    </p>
    <p>
    Suppose 
    \( M < \frac{1}{16} N^2. \)
    Then
    \(
        \sigma(A) = \sigma_1 \cup \sigma_2,
    \)
    \( \sigma_1\cap \sigma_2 = \emptyset, \)
    where \( \sigma_1 = \{ N - x_{11}^o \} \)
    and
    \(
        \lvert x_{11}^o \rvert,
        \ \sup_{\lambda\in\sigma_2} \lvert\lambda\rvert \leq 4\sqrt{M}.
    \)
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Moreover:
    \(
        \hat{h}_N = U(E+\Gamma X^o) e_1 = \)<br/>
    \( = h_N - \frac1N (X_{21,(1)}^o h_1 + \cdots + X_{21, (N{-}1)}^o h_{N{-}1}) \)
    is the principal eigenvector of $J_N - B$
    and
    \(
        \|\hat{h}_N - h_N\|_2 \leq 4\frac{\sqrt{M}}{N}.
    \)<br/>
    Here 
    \( X_{21,(i)}^o,\ i=\overline{1,N{-}1} \) are the coordinates
    of the vector \( X_{21}^o \).
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Now let \( A\in\mathbb{C}^{M{\times}M} \)
    and consider
    \(
        \mathbb{A} =
        \begin{pmatrix}
            A & \cdots & A \\
            \vdots & \ddots & \vdots \\
            A & \cdots & A
        \end{pmatrix}
        \in\mathbb{C}^{MN{\times}MN}
    \)
    and the perturbed matrix
    \(
        \mathbb{A} - \mathbb{B},\ \mathbb{B}\in\mathbb{C}^{MN{\times}MN}.
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Suppose \( A \) is invertible self-adjoint matrix.
    Then it has \( M \) orthonormal eigenvectors \( h_1, \ldots, h_M \)
    (\(\left\|h_i\right\|_2 = 1,\ i{=}\overline{1,M}\))
    with the corresponding eigenvalues
    \( \lambda_1, \ldots, \lambda_M \neq 0\).
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The spectrum of \( \mathbb{A} \) is
    \(
        \sigma(\mathbb{A}) = \{0\}\cup N\sigma(A) = \{0\} \cup \{N\lambda;\ \lambda\in\sigma(A) \}.
    \)
    Non-zero eigenvalues of \( \mathbb{A} \)
        have corresponding block eigenvectors:
    \(
        f_j = \frac{1}{\sqrt{N}} (h_j, \ldots, h_j)\in \mathbb{C}^{MN},\ j=\overline{1,M}.
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The null-space of \( \mathbb{A} \)
        has an orthonormal basis:
    \(
        f_{j,k} = \frac{1}{\sqrt{k(k+1)}}
        (
        \underbrace{e_j, \ldots, e_j}_{k\ \text{copies}},
        -ke_j,
        0, \ldots, 0
        ) \in\mathbb{C}^{MN{\times}M}
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The matrix \( \mathbb{A} \) is similar to a block-diagonal matrix:
    \[
        \mathcal{A} =
        \left(\begin{array}{c|c}
            \operatorname{diag}(N\lambda_1,\ldots,N\lambda_M) & \mathbf{0} \\ \hline
            \mathbf{0} & \mathbf{0}
        \end{array}\right)\in\mathbb{C}^{MN{\times}MN};
    \]
    The similarity transform is orthogonal
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    We will consider block matrices
    of the size \( MN{\times}MN \)
    in the form
    \[
    X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} & \cdots & x_{1M} \\
                \vdots & \ddots & \vdots \\
                x_{M1} & \cdots & x_{MM}
            \end{matrix} &
            \begin{matrix}
                x_{1,M+1} \\
                \vdots \\
                x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                x_{M+1,1} &
                \cdots &
                x_{M+1,M}
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right),
    \]
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    As usual:
\(
        J X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &  & 0 \\
                 & \ddots &  \\
                0 &  & x_{MM}
            \end{matrix} &
            \begin{matrix}
                0 \\
                \vdots \\
                0
            \end{matrix} \\ \hline
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right).
\)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Suppose \( A \)
    is of simple spectrum (i.e. has distinct eigenvalues).
    Then
    \(\tiny
        \Gamma X = 
            \frac1n \left(\begin{array}{c|c}
            \begin{matrix}
                0               & \gamma_{12}x_{12} & \cdots & \gamma_{1M}x_{1M} \\
                \gamma_{21}x_{21}  & 0              & \cdots & \gamma_{2M}x_{2M} \\
                \vdots          & \vdots         & \ddots & \vdots & \ \\
                \gamma_{M1}x_{M1}  & \gamma_{M2}x_{M2} & \cdots & 0
            \end{matrix} &
            \begin{matrix}
                \gamma_{1,M+1}x_{1,M+1} \\
                \gamma_{2,M+1}x_{2,M+1} \\
                \vdots \\
                \gamma_{M,M+1}x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                \gamma_{M{+}1,1}x_{M{+}1,1} &
                \gamma_{M{+}1,2}x_{M{+}1,2} &
                \cdots &
                \gamma_{M{+}1,M}x_{M{+}1,M}
            \end{matrix} &
            \mathbf{0}
        \end{array}\right),
    \)
    \(\tiny
        \gamma_{ij} = \left\{
            \begin{aligned}
                & \frac{1}{\lambda_i - \lambda_j},\ 1\leq i{\neq}j \leq M{+}1,\\
                & 0,\ i=j
            \end{aligned}
            \right.
    \)<br/>
    with the convention
    \(\tiny
        \lambda_{M{+}1} = 0.
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Then
    \(
        \|\Gamma\|_{\mathrm{op}} =
        \frac1N
        \frac{1}{\min\limits_{1\leq i{\neq}j \leq M{+}1}|\lambda_i - \lambda_j|} =
        \)<br/>
    \(
        = \frac1N
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|}},
         \frac{1}{
             \min\limits_{1\leq j \leq M}{|\lambda_j|}}
         \right\}.
    \)<br/>
    NB: the method of similar operators
    takes disjoint parts of spectrum
    and constructs a spectral projection for each.
    We've just relied on eigenvalues being distinct
    and our precision now depends
    on the smallest gap between two eigenvalues.
    Instead we could've taken the union
    of two eigenspaces.
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
Suppose \( A \) has simple spectrum and the following inequality holds:
\(
    \left\| \mathbb{B} \right\|_{\mathrm{op}}
        \leq 
        \frac{N}{4}
         \min\left\{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|},
             \min\limits_{1\leq j \leq M}{|\lambda_j|}
         \right\}.
 \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
Then the spectrum of disturbed matrix \( \mathbb{A} - \mathbb{B} \) is
\[
    \sigma\left(\mathbb{A}\right) =
        \left\{
            N\lambda_1 - x_{11}^o, \ldots, N\lambda_M - x_{MM}^o
        \right\}
    \cup \sigma_{M{+}1},
\]
\[
    \lvert x_{jj}^o\rvert,
    \ \max_{\lambda\in\sigma_{M{+}1}} \lvert\lambda\rvert
    \leq 4\|B\|.
\]
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The eigenvectors
    \( \hat{f}_j,\ \hat{f}_{j,k},\ j{=}\overline{1,M},\ k{=}\overline{1,N{-1}} \)
    of the perturbed matrix satisfy
    \[
    \left\| \hat{f}_j - f_j \right\|_2,
    \ \left\| \hat{f}_{j,k} - f_{j,k}\right\|_2
    \leq
    \]
    \[ \leq
    \frac4N \|B\|
    \max\left\{
    \frac{1}{
    \min\limits_{1\leq l{\neq}p \leq M }{|\lambda_l - \lambda_p|}},
    \frac{1}{
    \min\limits_{1\leq l \leq M}{|\lambda_l|}}
    \right\}
    \]
    for all \( j{=}\overline{1,M}, k{=}\overline{1,N-1} \).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
Now we consider Kronecker product
\[
    A\otimes B =
    \begin{pmatrix}
        a_{11} B & \cdots & a_{1N} B \\
        \vdots   & \ddots & \vdots \\
        a_{N1} B & \cdots & a_{NN} B
    \end{pmatrix}
    \in \mathbb{C}^{MN{\times}MN}
\]
of squared matrices
\( A={(a_{ij})}\in\mathbb{C}^{N{\times}N},
 \ B={(b_{ij})}\in\mathbb{C}^{M{\times}M}. \)
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    We will analyze its spectral properties
    under small-norm perturbations:
    \begin{equation}\label{nkjpcs-kronperturb}
    A\otimes B - F.
    \end{equation}
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
Suppose \( A \) and \( B \) have simple structure,
    i.e. \( A \) has \( N \) eigenvectors
    \( f_1, \ldots, f_N \)
    whose corresponding eigenvalues are \( \mu_1, \ldots, \mu_N \)
    and \( B \) has eigenvectors \( h_1, \ldots, h_M \)
    with the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
Then \( A\otimes B \) also has simple structure;
    it has \( MN \) independent eigenvectors \( f_i\otimes h_j,\ i{=}\overline{1,N}, j{=}\overline{1,M} \)
    and the corresponding eigenvalues are \( \mu_i \lambda_j \).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
Now suppose that among these pairwise products \( \mu_i \lambda_j \)
    there are only \( s \) distinct values \( \nu_1, \ldots, \nu_s \).
To each eigenvalue \( \nu_k \) (\( k{=}\overline{1,s} \)) there corresponds
    an eigenspace \[ E_k = \operatorname{span}(f_i\otimes h_j;\ \mu_i\lambda_j = v_k,\ i{=}\overline{1,N},\ j{=}\overline{1,M}) \subset \mathbb{C}^{MN}. \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
These eigenspace form a direct-sum decomposition of \( \mathbb{C}^{MN} \):
    \[ \mathbb{C}^{MN} = E_1 \oplus \cdots \oplus E_s. \]
Any vector \( x\in\mathbb{C}^{MN} \) can be uniquely represented
    in the form
    \begin{equation}\label{nkjpcs-decomposition-x}
        x = x_1 + \cdots + x_s,\ x_k\in E_k,\ k=\overline{1,s}.
    \end{equation}
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
This direct-sum decomposition of \( \mathbb{C}^{MN} \)
    corresponds to a decomposition of the identity matrix \( E\in \mathbb{C}^{MN{\times}MN} \)
    (which defines an identity operator)
    into a sum of matrices of the spectral projections:
    \[
        E = P_1 + \cdots + P_s.
    \]
The spectral projection \( \mathcal{P}_k \) (\(k{=}\overline{1,s}\)) is given by the formula
    \[
        \mathcal{P}_k x = x_k \in E_k\subset \mathbb{C}^{MN}
    \]
    with respect to the decomposition~\eqref{nkjpcs-decomposition-x} of \( x \).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
For any matrix \( X\in \mathbb{C}^{MN{\times}MN} \)
    the following trivial equality holds:
    \[
        X = \sum_{i,j=1}^s P_i X P_j.
    \]

The matrix \( A\otimes B \) can be decomposed into
    \[
        \mathcal{A} = \sum v_j P_j.
    \]

Now we should be able reproduce the same steps as before
    to retrieve the estimates.
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
The natural way to define \( J \) is as follows:
    \[
        JX = \sum_{j=1}^s P_j X P_j.
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
The system of equations
    \[\left\{\begin{aligned}
        & \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - JX, \\
        & J\Gamma X = 0,\ X\in \mathbb{C}^{MN{\times}MN}
    \end{aligned}\right.\]
    has the unique solution:
    \[
        \Gamma X = \sum_{1\leq i{\neq}j \leq s} \frac{1}{\nu_i-\nu_j} P_i X P_j.
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    The norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} = \gamma = \frac{1}{\min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert}
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    Suppose
    \[
        \|F\| \leq \frac14 \gamma^{-1} = \frac14 \min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert.
    \]
    Then \( A{\otimes}B - F \) is similar to
    \[ \sum_{k=1}^s \nu_k P_k - JX^o = \sum_{k=1}^s (\nu_k P_k - P_k X^o P_k) \]
    for some \( X^o \in \mathbb{C}^{MN{\times}MN} \),
    \( \|X^o - F\|\leq 3\|F\| \).

    All the eigenvalues of \( A{\otimes}B - F \) are contained in the circles
    \[
        \Omega_k = \left\{
            \lambda\in\mathbb{C};
            \ \lvert\lambda - \nu_k\rvert \leq 4\|F\|
            \right\},
        \ k{=}\overline{1,s}.
    \]
    There is at least one eigenvalue in each of these circles.
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    Suppose the eigenvalue \( \nu_k=\mu_{i_k}\lambda_{j_k} \) of \( A{\otimes}B \) has multiplicity \( 1 \),
        that is it has the only eigenvector \( v_k = f_{i_k}{\otimes}h_{j_k} \).
    It is equivalent to the statement that the eigenvalue \( \mu_{i_k} \)
        of \( A \) and the eigenvalue \( \lambda_{j_k} \) of \( B \)
        are both of multiplicity \( 1 \).
    Then \( A{\otimes}B - F \) has eigenvalue in the circle \( \Omega_k \)
        and the corresponding eigenvector \( \hat{v}_k \) is within bounds
    \[
        \|\hat{v}_k - v_k\| \leq 4\gamma \|F\|.
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    If \( \nu_k \) is well separated from all the other eigenvalues of \( A{\otimes}B \):
    \[
        \min_{l\neq k}
        \lvert
        \nu_k - \nu_l
        \rvert
        \geq 4\|F\|,
    \]
    then \( \nu_k \) is the only eigenvalue of \( A{\otimes}B - F \)
    in that circle.
    </p>
</section>

<section>
    <h3>Kronecker products</h3>

    <p>
For example, in the case of the already-mentioned "tiled" matrix
\(
    J_N{\otimes}B =
    \begin{pmatrix}
        B & \cdots & B \\
        \vdots & \ddots & \vdots \\
        B & \cdots & B
    \end{pmatrix}
\)<br/>
    we would have
    \( \nu_1=N \),
    \( \nu_2=0 \).
Let \( \lambda_1,\ldots,\lambda_M \)
    be the eigenvalues of \( B \).
The spectrum of \( J_N{\otimes}B \) is
    \(
        \sigma(J_N{\otimes}B) = \left\{ \mu_i\lambda_j;\ i{=}\overline{1,2},\ j{=}\overline{1,M}\right\} = \{0\}\cup N\sigma(B). \)
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
All of these eigenvalues except for \( 0 \)
    are of multiplicity \( 1 \)
    and are well-separated for sufficiently large \( N \).
Then \( \gamma=\frac1N \).
This directly implies the theorem of the previous section.
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
These results might be refined
    with the use of the theorem on splitting an operator~\cite{baskakov1987theorem}
    which allows to consider each eigenvalue individually
    and obtain more precise estimates for the corresponding
    eigenvalue of the perturbed matrix.
    </p>
</section>
<section>
    <h3>References</h3>

    <p>TODO</p>
</section>
<section>
    <h2>The method of similar operators in the study of spectra of the adjacency matrices of graphs</h2>

    {% include my_contacts.html %}
</section>
