---
title: "CONF2017: The method of similar operators in the study of spectra of the adjacency matrices of graphs"
categories: math
layout: slides
slides_theme: white
---

<section>
    <h2>The method of similar operators in the study of spectra of the adjacency matrices of graphs</h2>

    {% include my_contacts.html %}
</section>
<section>
    <h3>Goals of the talk</h3>
    <ul>
        <li>Show that graphs' spectra are cool.</li>
        <li>Show that the method of similar operators is cool.</li>
    </ul>
</section>
<section>
    <h3>Structure of the talk</h3>

    <ul>
        <li>Which graphs exactly?</li>
        <li>Which spectra?</li>
        <li>Why?</li>
        <li>What's the method of similar operators?</li>
        <li>What are the results?</li>
    </ul>
</section>

<section>
    <h3>Which graphs?</h3>
    <p>
    A "graph" hereafter is a simple directed graph
    with the finite set of vertices $V=\left\{1,\ldots,N\right\}$
    and the set of directed edges $E \subset V^2$.
    </p>
</section>
<section>
    <h3>Adjacency matrices</h3>
    <p>
    Such a graph is uniquely defined by its <i>adjacency matrix</i>
    $$A = (a_{ij}),\ i,j{=}\overline{1,N},$$
    where $a_{ij}$
    is the number of directed edges
    from vertex $j$ to vertex $i$.<br/>
    Note the inversion of indices.
    </p>
</section>
<section>
    <h3>Simple graphs</h3>
    <p>
    Sometimes we'll restrict ourselves to <i>simple graphs</i>:
    the ones without loops and multiple edges.
    In terms of adjacency matrix:
    $$a_{ij}\in \{0,1\},\ a_{ii} = 0,$$
    for $i,j = \overline{1,N}.$
    </p>
</section>
<section>
    <h3>Which spectra?</h3>

    <p>
    We are interested in
    </p>
    <ul>
        <li>the spectrum of the adjacency matrix $A$</li>
        <li>and the spectra of transition matrices of some random processes.</li>
    </ul>
    <p class="fragment">
    We study in an abstract setting
    the spectra of the perturbed Kronecker products
    of matrices.
    </p>
</section>
<section>
    <h3>The (Bonacich) eigenvector centrality</h3>

    <p>Consider a problem of ranking
    scientific papers or web-pages
    that reference each other.</p>
    <ol>
        <li>The more links to a page the greater importance.</li>
        <li class="fragment">The more important referrers the greater importance.</li>
        <li class="fragment">Iterate.</li>
        <li class="fragment">$$x_j = c \sum_i a_{ik} x_k.$$</li>
    </ol>
</section>
<section>
    <p>In more convenient form:
    $$A x = \lambda x.$$</p>
    <p class="fragment">
    For the importances to be non-negative
    a principal eigenvector is chosen.<br/>
    That is one corresponding to the largest (the Perron-Frobenius) eigenvalue.</p>
    <p class="fragment">For a strongly-connected simple graph
    the adjacency matrix is always irreducible and Perron-Frobenius applies.</p>
</section>
<section>
    <h3>Google's PageRank: random surfer interpretation</h3>

    <p>An alternative approach to define eigenvector centrality.
    Consider an agent visiting a web page
    and then randomly following one of the links.</p>
    <p class="fragment">It's a Markovian random walk with the probability
    of transition from state (web-page) $j$
    to state $i$ equal to $$p_{ij} = \frac{a_{ij}}{D_j}.$$
    Here $D_j = \sum_k a_{kj}$ is the out-degree of the node $j$.</p>
</section>
<section>
    <h3>Random surfer interpretation</h3>

    <p>
    Let $D=\mathrm{diag}(d_1,\ldots d_N)$.</p>
    <p class="fragment">
    Then $P=AD^{-1}$ is the (column-stochastic)
    transition matrix of this process.</p>
    <p class="fragment">
    Its Perron-Frobenius eigenvalue is $1$.
    It has a non-negative eigenvector
    that defines a stationary distribution:
    $$x = P x,$$
    $x\geq 0,\ \sum_j x_j = 1$.
    </p>
</section>
<section>
    Interpretation:
    the fraction of time a random surfer would spend in each node</p>
</section>
<section>
    <h3>The second eigenvalue of $P$</h3>

    <p>Customarily the eigenvector centrality is computed
    using the power iteration method.
    For a <b>positive</b> transition matrix $P$ (Perron-Frobenius):
    <ol>
        <li>Start at arbitrary $x^{(0)}\neq 0$.</li>
        <li>Set
            $x^{(n)} = \frac{1}{\|x^{(n-1)}\|} P x^{(n-1)},\ n\in\mathbb{N}.$
        </li>
    </ol>
    </p>
    <p>
    The process converges to a strictly positive eigenvector $x$
    which corresponds to the eigenvalue $1$.
    This eigenvalue is simple and equals the spectral radius of $P$.
    </p>
</section>
<section>
    <h3>The second eigenvalue of $P$</h3>

    <p>
    First, it is obvious that for positive $P$:
    $$\|x - x^{(n)}\| = O(\lvert\frac{\lambda_2}{\lambda_1}\rvert^n) = O(\lvert\lambda_2\rvert^n),$$
    where $\lambda_2$ is the second-largest eigenvalue of $P$.<br/>
    In other words the speed of convergence is determined by $\lambda_2$.</p>
    <p class="fragment">
    Meyer (1994) also shows that (for non-negative $P$)
    the sensitivity of stationary distributions
    to perturbations in P
    is determined by the gap
    between the two largest eigenvalues.
    </p>
</section>
<section>
    <h3>Convex combinations</h3>

    <p>The Page's paper also deals with the disconnectedness of the graph of web.
    It is done by allowing an agent (a random surfer)
    to reset and jump to an arbitrary page
    with some probability $\alpha$.
    </p>
    <p class="fragment">
    The transition matrix for this new process is:
    $$\mathcal{H} = (1-\alpha)P + \alpha J,$$
    where $J$ is the all-ones matrix.
    </p>
    <p class="fragment">
    This matrix is positive and thus the Perron-Frobenius theorem
    and the power method are applicable!
    </p>
</section>
<section>
    <h3>Viral spread.</h3>

    <p>
    The Susceptible-Infective-Susceptible model
    describes viral spread in a network on $N$ nodes
    as a Markov process with $2^N$ states.
    Specifically, state is a vector of nodes' statuses
    (each node is either infected or susceptible to infection).<br/>
    </p>
</section>
<section>
    <h3>Viral spread.</h3>

    <p>
    Wang et al. investigated ergodicity of such a system
    in the following case:
    </p>
    <ul>
        <li>An infected node gets cured with the probability $\delta$.</li>
        <li>A healthy neighbour of infective node gets infected w.p. $\beta$.</li>
    </ul>
</section>
<section>
    <h3>Viral spread.</h3>

    <p>
    Turns out (Wang et. al):
    $$\mathrm{Pr}\left\{\text{node}\ i\ \text{is infected at the step}\ n\right\} \to 0,\ \text{as}\ n\to\infty,$$
    for all $i$
    if (and only if) $$\frac{\beta}{\delta} < \frac{1}{\mathrm{spr}A}.$$
    Moreover this probability decays in exponential manner.
    </p>
    <p>
    Otherwise infection lingers for ever.
    </p>
</section>
<section>
    <h3>Viral spread.</h3>
    <p>
    TLDR: the spectral radius of the adjacency matrix
    represents the level of inter-connectedness of a network.
    </p>
</section>
<section>
        <h3>Kronecker products</h3>

        <p>
        One of the most general operations on graphs
        <i>is the non-complete extended p-sum</i> (NEPS) is defined as follows (see Cvetkovic):
        </p>
        <p class="fragment">
        The NEPS of graphs $G_1,\ldots, G_n$
        with basis $\mathcal{B}\subset \{0,1\}^n\setminus 0$
        is the graph with vertex set $V(G_1)\times\cdots\times V(G_n)$,
        in which a vertex $u=(u_1,\ldots,u_n)$ is linked to a vertex $v=(v_1,\ldots,v_n)$
        IFF there is $\beta=(\beta_1,\ldots,\beta_n)\in\mathcal{B}$
        such that
        $u_i=v_i$ whenever $\beta_i=0$
        and $u_i$ is linked to $v_i$ in $G_i$ whenever $\beta_i=1$.
        </p>
</section>
<section>
    <h3>Kronecker products</h3>
    
    <p>
    NEPS can serve a basis for some other important operations.
    </p>
    <ul>
        <li>The cartesian product of two graphs: $\mathcal{B}=\{(0,1),\ (1,0)\}$.</li>
        <li>The strong product: $\mathcal{B}=\{ (0,1),\ (1, 0),\ (1,1) \}$.</li>
        <li>etc.</li>
    </ul>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    The adjacency matrix of the NEPS of $G_1,\ldots,G_n$
    with basis $\mathcal{B}$ is
    $$A = \sum_{\beta\in\mathcal{B}} A_1^{\beta_1} \otimes\cdots\otimes A_n^{\beta_n}$$
    where $A_1,\ldots,A_n$ are adjacency matrices of $G_1,\ldots, G_n$ resp.
    and $\otimes$ denotes the Kronecker product (see below).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    The Kronecker product of two matrices $A=(a_{ij})\in\mathbb{C}^{m{\times}n}$
    and $B=(b_{ij})\in\mathbb{C}^{p{\times}q}$ is the matrix of the size $mn{\times}pq$
    defined as:
    $$A{\otimes}B =
    \begin{pmatrix}
    a_{11} B & \cdots & a_{1n} B \\
    \vdots & \ddots & \vdots \\
    a_{m1} B & \cdots & a_{mn} B \\
    \end{pmatrix}.$$
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>Kronecker product has some convenient properties:</p>
    <ul>
        <li> It is associative:
            $A\otimes (B\otimes C) = (A\otimes B)\otimes C.$</li>
        <li class="fragment"> It is distributive w.r.t. addition:
            $(A+B)\otimes(C+D) = A\otimes C + A\otimes D +$</br>$+ B\otimes C + B\otimes D.$</li>
        <li class="fragment">$(AB)\otimes(CD) = (A\otimes C)(B\otimes D)$
            whenever products \( AB \) and \( CD \) make sense.
            (I.e. it corresponds to tensor products of operators)
        </li">
        <li class="fragment">
            $\operatorname{tr}(A\otimes B) = \operatorname{tr}A\operatorname{tr}B$.</li>
        <li class="fragment"> $A$ and $B$ are symmetric $\implies$
            $A\otimes B$ is symmetric.</li>
    </ul>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>The method of similar operators
    is a means to track eigenvalues
    and spectral projections
    of small-norm perturbations of some "ideal" operator.
    </p>
    <p class="fragment">
    The method originates from Friedrichs (1965)
    and was later developed in abstract setting
    by Anatoly Baskakov (1983, 2013, 2017).
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Unlike usual perturbation theory methods
    which rely on series expansions,
    the method of similar operators
    makes use of fixed-point theorem
    in appropriate Banach space.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    First we introduce some notation.
    While the abstract method of similar operators
    was developed for Banach spaces,
    we will for the purposes of this talk,
    account for finite-dimensionality and
    put everything into matrix form.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Two matrices \( A, B \)
    are called <i>similar</i>
    if there is an invertible matrix \( U \)
    (the similarity matrix)
    such that \( A U = U B \).

    Similar matrices have identical spectrum
    and $U$ transforms eigenvectors of one to anothers':
    $Bx = \lambda x \implies A Ux = \lambda Ux$.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    We'll make use of the following spaces:
    </p>
    <ul>
    <li>The Euclid space $\mathbb{C}^{n}$ with the $L_2$-norm.</li>
    <li>The Banach algebra $L(V)$ of (bounded) linear operators $V\to V$
        defined on a Banach space $V$
        with the operator norm:
    \(
        \|A\|_{\mathrm{op}} =
        \sup_{
            \substack{\|x\|=1,\\ x\in V}
        } \|A x\|,\ A\in L(V).
        \)
    </ul>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>The space of matrices $\mathbb{C}^{n{\times}n}$
        of the size $n{\times}n$ with complex entries
        and one of the following norms:
        $\|A\|_{\mathrm{F}} = \sqrt{\sum_{ij} |a_{ij}|^2},$
        $\|A\|_{\mathrm{op}} =
        \sup_{
        \substack{\|x\|_2=1,\\ x\in \mathbb{C}^n}
            } \|A x\|_2,\ A\in \mathbb{C}^{m{\times}n}.$
    </p>
    <p class="fragment">
    NB: $\mathbb{C}^{n{\times}n}$ supplied
    with the operator norm $\|\cdot\|_{\mathrm{op}}$
    is isomorphic to the space $L(\mathbb{C}^n)$
    of linear operators $\mathbb{C}^{n}\to\mathbb{C}^{n}$.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    We will also consider the space
    $L(L(V))$
    of linear transformations of operators
    which we will refer to as "transformers" (following MG Krein).<br/>
    We will supply this space with operator norm
    corresponding to the norm in $L(V)$.<br/>
    NB: $L(L(\mathbb{C}^n))$ is isomorphic to $L(\mathbb{C}^{n{\times}n})$.
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>The formulation of the main theorem of the method of similar operators
    is built around the notion of an <i>admissible triple</i>
    (which effectively becomes a pair in finite-dimensional spaces).
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Consider a matrix $A\in \mathbb{C}^n$.
    A tuple \( (\mathbb{C}^{n{\times}n}, J, \Gamma) \)
    is said form an <i>admissible triple</i>
    for the matrix $A$ (Baskakov) if
    <ol>
        <li>\( J, \Gamma \in L(\mathbb{C}^{n{\times}n}) \)
        are transformers.</li>
        <li>\( J \) is a projection (\( J^2 = J \)).</li>
        <li>
            $\Gamma$ satisfies<br/>
        \(
            A \Gamma X - (\Gamma X) A = X - JX,
        \)<br/>
        \(
            J\Gamma X = 0,
        \)
        for all $X\in\mathbb{C}^{n{\times}n}$.
        </li>
    </ol>
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Consider a matrix \( A - B \),
        with \( A, B \in \mathbb{C}^{n{\times}n} \).
    Suppose \( (\mathbb{C}^{n{\times}n}, J, \Gamma) \)
        is admissible for \( A \)
        and suppose
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    Then there exists such a \( X^o\in\mathbb{C}^{n{\times}n} \)
        that \( A - B \) is similar to \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|,
        \]
    </p>
</section>
<section>
    <h3>The method of similar operators</h3>

    <p>
    \( X^o \) is the limit of a convergent sequence
        \( \left( \Phi^k(0);\ k\in\mathbb{N} \right) \)
        in a Banach algebra \( \mathbb{C}^{n{\times}n} \).
        Here \( \Phi \) is a non-linear contraction mapping
        defined in the ball \( \{X\in\mathbb{C}^{m{\times}n};\ \|X-B\|\leq 3\|B\| \} \)
        and given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B.
    \]
        Here \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes composition.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Consider a digraph defined by the following adjacency matrix:
    \[
        A = J_N - B =
        \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - B,
    \]
The unity on the intersection
of \( i \)'th row and \( j \)'th column of \( B \)
corresponds to an edge from \( j \) to \( i \)
being absent in the graph.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
The minimal annihilating polynomial of \( J_N \)
    is \( \lambda(\lambda - N) \).
This comes from the fact that \( J_N^2 = N J_N \).
Consequently the spectrum of \( J_N \) is
\[
    \sigma(J_N) = \left\{0, N\right\}.
\]
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
The only non-zero eigenvalue \( N \) has the corresponding eigenvector
\[
    h_N = \frac{1}{\sqrt{N}} \left(1, \ldots, 1\right)\in\mathbb{R}^N.
\]
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
The null-space of \( J_N \) is orthogonal to \( h_N \)
    and allows the orthonormal basis:
\[
    h_k = \frac{1}{\sqrt{k(k+1)}} \left(\underbrace{1, \ldots, 1}_{k\ \text{copies}}, -k, 0, \ldots, 0\right),
\]
for \( k=\overline{1, N-1} \)
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
We conclude that the adjacency matrix \( A = J_N - B \) of this graph
    is similar to \( \mathcal{A} - \mathcal{B} \)
    where
    \(
        \mathcal{A} = \left(\begin{array}{c|c}
        N & \mathbf{0}_{1{\times}(N{-}1)} \\ \hline
            \mathbf{0}_{(N{-}1){\times}1} & \mathbf{0}_{(N{-}1){\times}(N{-}1)}
        \end{array}\right) \in \mathbb{R}^{N{\times}N},
    \)
    and
    \(
        \mathcal{B} = U^{-1} B U \in \mathbb{R}^{N{\times}N}.
    \)
The similarity matrix \( U \) is given by stacking the eigenvectors in columns.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
Following the general method,
    we should first construct an admissible triple.
It is natural to set
    \[
        JX =
        \left(\begin{array}{c|c}
            x_{11} & \mathbf{0} \\ \hline
            \mathbf{0} & X_{22}
        \end{array}\right)
    \]
    </p>
    <p class="fragment">
Then \( A - JX \) is block-diagonal for any \( X \)
    and its spectrum is the union
    of its diagonal blocks' spectra:
    \( \sigma(A - JX) = \{N, \sigma(-X_{22}) \} \).
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Then (by definition of an admissible triple)
    \[
        \Gamma X = \frac{1}{N}
        \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix},\ X\in\mathbb{C}^{N{\times}N}.
    \]

    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Then we estimate the norms:
    \( \|\Gamma\|_{\mathrm{op}} = \frac1N \).
    \( \|\mathcal{B}\|_{\mathrm{op}} =
    \|B\|_{\mathrm{op}} \leq
    \|B\|_{\mathrm{F}} =
    \sqrt{\sum_{ij} b_{ij}^2} = M \)<br/>
    Recall: $M$ is the number of absent edges.
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Finally we have the result:
    </p>
    <p>
    Suppose 
    \( M < \frac{1}{16} N^2. \)
    Then
    \(
        \sigma(A) = \sigma_1 \cup \sigma_2,
    \)
    \( \sigma_1\cap \sigma_2 = \emptyset, \)
    where \( \sigma_1 = \{ N - x_{11}^o \} \)
    and
    \(
        \lvert x_{11}^o \rvert,
        \ \sup_{\lambda\in\sigma_2} \lvert\lambda\rvert \leq 4\sqrt{M}.
    \)
    </p>
</section>
<section>
    <h3>The results: almost-complete graph</h3>

    <p>
    Moreover:
    \(
        \hat{h}_N = U(E+\Gamma X^o) e_1 = \)<br/>
    \( = h_N - \frac1N (X_{21,(1)}^o h_1 + \cdots + X_{21, (N{-}1)}^o h_{N{-}1}) \)
    is the principal eigenvector of $J_N - B$
    and
    \(
        \|\hat{h}_N - h_N\|_2 \leq 4\frac{\sqrt{M}}{N}.
    \)<br/>
    Here 
    \( X_{21,(i)}^o,\ i=\overline{1,N{-}1} \) are the coordinates
    of the vector \( X_{21}^o \).
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Now let \( A\in\mathbb{C}^{M{\times}M} \)
    and consider
    \(
        \mathbb{A} =
        \begin{pmatrix}
            A & \cdots & A \\
            \vdots & \ddots & \vdots \\
            A & \cdots & A
        \end{pmatrix}
        \in\mathbb{C}^{MN{\times}MN}
    \)
    and the perturbed matrix
    \(
        \mathbb{A} - \mathbb{B},\ \mathbb{B}\in\mathbb{C}^{MN{\times}MN}.
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Suppose \( A \) is invertible self-adjoint matrix.
    Then it has \( M \) orthonormal eigenvectors \( h_1, \ldots, h_M \)
    (\(\left\|h_i\right\|_2 = 1,\ i{=}\overline{1,M}\))
    with the corresponding eigenvalues
    \( \lambda_1, \ldots, \lambda_M \neq 0\).
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The spectrum of \( \mathbb{A} \) is
    \(
        \sigma(\mathbb{A}) = \{0\}\cup N\sigma(A) = \{0\} \cup \{N\lambda;\ \lambda\in\sigma(A) \}.
    \)
    Non-zero eigenvalues of \( \mathbb{A} \)
        have corresponding block eigenvectors:
    \(
        f_j = \frac{1}{\sqrt{N}} (h_j, \ldots, h_j)\in \mathbb{C}^{MN},\ j=\overline{1,M}.
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The null-space of \( \mathbb{A} \)
        has an orthonormal basis:
    \(
        f_{j,k} = \frac{1}{\sqrt{k(k+1)}}
        (
        \underbrace{e_j, \ldots, e_j}_{k\ \text{copies}},
        -ke_j,
        0, \ldots, 0
        ) \in\mathbb{C}^{MN{\times}M}
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The matrix \( \mathbb{A} \) is similar to a block-diagonal matrix:
    \[
        \mathcal{A} =
        \left(\begin{array}{c|c}
            \operatorname{diag}(N\lambda_1,\ldots,N\lambda_M) & \mathbf{0} \\ \hline
            \mathbf{0} & \mathbf{0}
        \end{array}\right)\in\mathbb{C}^{MN{\times}MN};
    \]
    The similarity transform is orthogonal
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    We will consider block matrices
    of the size \( MN{\times}MN \)
    in the form
    \[
    X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} & \cdots & x_{1M} \\
                \vdots & \ddots & \vdots \\
                x_{M1} & \cdots & x_{MM}
            \end{matrix} &
            \begin{matrix}
                x_{1,M+1} \\
                \vdots \\
                x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                x_{M+1,1} &
                \cdots &
                x_{M+1,M}
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right),
    \]
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    As usual:
\(
        J X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &  & 0 \\
                 & \ddots &  \\
                0 &  & x_{MM}
            \end{matrix} &
            \begin{matrix}
                0 \\
                \vdots \\
                0
            \end{matrix} \\ \hline
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix} &
            X_{M+1,M+1}
        \end{array}\right).
\)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Suppose \( A \)
    is of simple spectrum (i.e. has distinct eigenvalues).
    Then
    \(\tiny
        \Gamma X = 
            \frac1n \left(\begin{array}{c|c}
            \begin{matrix}
                0               & \gamma_{12}x_{12} & \cdots & \gamma_{1M}x_{1M} \\
                \gamma_{21}x_{21}  & 0              & \cdots & \gamma_{2M}x_{2M} \\
                \vdots          & \vdots         & \ddots & \vdots & \ \\
                \gamma_{M1}x_{M1}  & \gamma_{M2}x_{M2} & \cdots & 0
            \end{matrix} &
            \begin{matrix}
                \gamma_{1,M+1}x_{1,M+1} \\
                \gamma_{2,M+1}x_{2,M+1} \\
                \vdots \\
                \gamma_{M,M+1}x_{M,M+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                \gamma_{M{+}1,1}x_{M{+}1,1} &
                \gamma_{M{+}1,2}x_{M{+}1,2} &
                \cdots &
                \gamma_{M{+}1,M}x_{M{+}1,M}
            \end{matrix} &
            \mathbf{0}
        \end{array}\right),
    \)
    \(\tiny
        \gamma_{ij} = \left\{
            \begin{aligned}
                & \frac{1}{\lambda_i - \lambda_j},\ 1\leq i{\neq}j \leq M{+}1,\\
                & 0,\ i=j
            \end{aligned}
            \right.
    \)<br/>
    with the convention
    \(\tiny
        \lambda_{M{+}1} = 0.
    \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    Then
    \(
        \|\Gamma\|_{\mathrm{op}} =
        \frac1N
        \frac{1}{\min\limits_{1\leq i{\neq}j \leq M{+}1}|\lambda_i - \lambda_j|} =
        \)<br/>
    \(
        = \frac1N
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|}},
         \frac{1}{
             \min\limits_{1\leq j \leq M}{|\lambda_j|}}
         \right\}.
    \)<br/>
    NB: the method of similar operators
    takes disjoint parts of spectrum
    and constructs a spectral projection for each.
    We've just relied on eigenvalues being distinct
    and our precision now depends
    on the smallest gap between two eigenvalues.
    Instead we could've taken the union
    of two eigenspaces.
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
Suppose \( A \) has simple spectrum and the following inequality holds:
\(
    \left\| \mathbb{B} \right\|_{\mathrm{op}}
        \leq 
        \frac{N}{4}
         \min\left\{
             \min\limits_{1\leq i{\neq}j \leq M }{|\lambda_i - \lambda_j|},
             \min\limits_{1\leq j \leq M}{|\lambda_j|}
         \right\}.
 \)
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
Then the spectrum of disturbed matrix \( \mathbb{A} - \mathbb{B} \) is
\[
    \sigma\left(\mathbb{A}\right) =
        \left\{
            N\lambda_1 - x_{11}^o, \ldots, N\lambda_M - x_{MM}^o
        \right\}
    \cup \sigma_{M{+}1},
\]
\[
    \lvert x_{jj}^o\rvert,
    \ \max_{\lambda\in\sigma_{M{+}1}} \lvert\lambda\rvert
    \leq 4\|B\|.
\]
    </p>
</section>
<section>
    <h3>"Tiled" matrix example</h3>

    <p>
    The eigenvectors
    \( \hat{f}_j,\ \hat{f}_{j,k},\ j{=}\overline{1,M},\ k{=}\overline{1,N{-1}} \)
    of the perturbed matrix satisfy
    \[
    \left\| \hat{f}_j - f_j \right\|_2,
    \ \left\| \hat{f}_{j,k} - f_{j,k}\right\|_2
    \leq
    \]
    \[ \leq
    \frac4N \|B\|
    \max\left\{
    \frac{1}{
    \min\limits_{1\leq l{\neq}p \leq M }{|\lambda_l - \lambda_p|}},
    \frac{1}{
    \min\limits_{1\leq l \leq M}{|\lambda_l|}}
    \right\}
    \]
    for all \( j{=}\overline{1,M}, k{=}\overline{1,N-1} \).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
Now we consider Kronecker product
\[
    A\otimes B =
    \begin{pmatrix}
        a_{11} B & \cdots & a_{1N} B \\
        \vdots   & \ddots & \vdots \\
        a_{N1} B & \cdots & a_{NN} B
    \end{pmatrix}
    \in \mathbb{C}^{MN{\times}MN}
\]
of squared matrices
\( A={(a_{ij})}\in\mathbb{C}^{N{\times}N},
 \ B={(b_{ij})}\in\mathbb{C}^{M{\times}M}. \)
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    We will analyze its spectral properties
    under small-norm perturbations:
    \begin{equation}\label{nkjpcs-kronperturb}
    A\otimes B - F.
    \end{equation}
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
Suppose \( A \) and \( B \) have simple structure,
    i.e. \( A \) has \( N \) eigenvectors
    \( f_1, \ldots, f_N \)
    whose corresponding eigenvalues are \( \mu_1, \ldots, \mu_N \)
    and \( B \) has eigenvectors \( h_1, \ldots, h_M \)
    with the eigenvalues \( \lambda_1, \ldots, \lambda_M \).
Then \( A\otimes B \) also has simple structure;
    it has \( MN \) independent eigenvectors \( f_i\otimes h_j,\ i{=}\overline{1,N}, j{=}\overline{1,M} \)
    and the corresponding eigenvalues are \( \mu_i \lambda_j \).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
Now suppose that among these pairwise products \( \mu_i \lambda_j \)
    there are only \( s \) distinct values \( \nu_1, \ldots, \nu_s \).
To each eigenvalue \( \nu_k \) (\( k{=}\overline{1,s} \)) there corresponds
    an eigenspace \[ E_k = \operatorname{span}(f_i\otimes h_j;\ \mu_i\lambda_j = v_k,\ i{=}\overline{1,N},\ j{=}\overline{1,M}) \subset \mathbb{C}^{MN}. \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
These eigenspace form a direct-sum decomposition of \( \mathbb{C}^{MN} \):
    \[ \mathbb{C}^{MN} = E_1 \oplus \cdots \oplus E_s. \]
Any vector \( x\in\mathbb{C}^{MN} \) can be uniquely represented
    in the form
    \begin{equation}\label{nkjpcs-decomposition-x}
        x = x_1 + \cdots + x_s,\ x_k\in E_k,\ k=\overline{1,s}.
    \end{equation}
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
This direct-sum decomposition of \( \mathbb{C}^{MN} \)
    corresponds to a decomposition of the identity matrix \( E\in \mathbb{C}^{MN{\times}MN} \)
    (which defines an identity operator)
    into a sum of matrices of the spectral projections:
    \[
        E = P_1 + \cdots + P_s.
    \]
The spectral projection \( \mathcal{P}_k \) (\(k{=}\overline{1,s}\)) is given by the formula
    \[
        \mathcal{P}_k x = x_k \in E_k\subset \mathbb{C}^{MN}
    \]
    with respect to the decomposition~\eqref{nkjpcs-decomposition-x} of \( x \).
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
For any matrix \( X\in \mathbb{C}^{MN{\times}MN} \)
    the following trivial equality holds:
    \[
        X = \sum_{i,j=1}^s P_i X P_j.
    \]

The matrix \( A\otimes B \) can be decomposed into
    \[
        \mathcal{A} = \sum v_j P_j.
    \]

Now we should be able reproduce the same steps as before
    to retrieve the estimates.
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
The natural way to define \( J \) is as follows:
    \[
        JX = \sum_{j=1}^s P_j X P_j.
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
The system of equations
    \[\left\{\begin{aligned}
        & \mathcal{A}\Gamma X - (\Gamma X) \mathcal{A} = X - JX, \\
        & J\Gamma X = 0,\ X\in \mathbb{C}^{MN{\times}MN}
    \end{aligned}\right.\]
    has the unique solution:
    \[
        \Gamma X = \sum_{1\leq i{\neq}j \leq s} \frac{1}{\nu_i-\nu_j} P_i X P_j.
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    The norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} = \gamma = \frac{1}{\min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert}
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    Suppose
    \[
        \|F\| \leq \frac14 \gamma^{-1} = \frac14 \min_{1\leq i{\neq}j\leq s}\lvert\nu_i - \nu_j\rvert.
    \]
    Then \( A{\otimes}B - F \) is similar to
    \[ \sum_{k=1}^s \nu_k P_k - JX^o = \sum_{k=1}^s (\nu_k P_k - P_k X^o P_k) \]
    for some \( X^o \in \mathbb{C}^{MN{\times}MN} \),
    \( \|X^o - F\|\leq 3\|F\| \).

    All the eigenvalues of \( A{\otimes}B - F \) are contained in the circles
    \[
        \Omega_k = \left\{
            \lambda\in\mathbb{C};
            \ \lvert\lambda - \nu_k\rvert \leq 4\|F\|
            \right\},
        \ k{=}\overline{1,s}.
    \]
    There is at least one eigenvalue in each of these circles.
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    Suppose the eigenvalue \( \nu_k=\mu_{i_k}\lambda_{j_k} \) of \( A{\otimes}B \) has multiplicity \( 1 \),
        that is it has the only eigenvector \( v_k = f_{i_k}{\otimes}h_{j_k} \).
    It is equivalent to the statement that the eigenvalue \( \mu_{i_k} \)
        of \( A \) and the eigenvalue \( \lambda_{j_k} \) of \( B \)
        are both of multiplicity \( 1 \).
    Then \( A{\otimes}B - F \) has eigenvalue in the circle \( \Omega_k \)
        and the corresponding eigenvector \( \hat{v}_k \) is within bounds
    \[
        \|\hat{v}_k - v_k\| \leq 4\gamma \|F\|.
    \]
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
    If \( \nu_k \) is well separated from all the other eigenvalues of \( A{\otimes}B \):
    \[
        \min_{l\neq k}
        \lvert
        \nu_k - \nu_l
        \rvert
        \geq 4\|F\|,
    \]
    then \( \nu_k \) is the only eigenvalue of \( A{\otimes}B - F \)
    in that circle.
    </p>
</section>

<section>
    <h3>Kronecker products</h3>

    <p>
For example, in the case of the already-mentioned "tiled" matrix
\(
    J_N{\otimes}B =
    \begin{pmatrix}
        B & \cdots & B \\
        \vdots & \ddots & \vdots \\
        B & \cdots & B
    \end{pmatrix}
\)<br/>
    we would have
    \( \nu_1=N \),
    \( \nu_2=0 \).
Let \( \lambda_1,\ldots,\lambda_M \)
    be the eigenvalues of \( B \).
The spectrum of \( J_N{\otimes}B \) is
    \(
        \sigma(J_N{\otimes}B) = \left\{ \mu_i\lambda_j;\ i{=}\overline{1,2},\ j{=}\overline{1,M}\right\} = \{0\}\cup N\sigma(B). \)
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
All of these eigenvalues except for \( 0 \)
    are of multiplicity \( 1 \)
    and are well-separated for sufficiently large \( N \).
Then \( \gamma=\frac1N \).
This directly implies the theorem of the previous section.
    </p>
</section>
<section>
    <h3>Kronecker products</h3>

    <p>
These results might be refined
    with the use of the theorem on splitting an operator~\cite{baskakov1987theorem}
    which allows to consider each eigenvalue individually
    and obtain more precise estimates for the corresponding
    eigenvalue of the perturbed matrix.
    </p>
</section>
<section>
    <h3>Conclusion</h3>

    <ol>
        <li>Love graphs' spectra.</li>
        <li>Love Banach spaces.</li>
        <li>Prefer contraction theory to series expansions.</li>
    </ol>
</section>
<section>
    <h3>References</h3>

    <p>TODO</p>
</section>
<section>
    <h2>The method of similar operators in the study of spectra of the adjacency matrices of graphs</h2>

    {% include my_contacts.html %}
</section>
